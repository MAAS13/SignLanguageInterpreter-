{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import time \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import random_split\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1=torch.load('train_data_50_mut_1_c.pt')\n",
    "test_data_1=torch.load('test_data_50_mut_1_c.pt')\n",
    "train_labels_1=torch.load('train_labels_50_mut_1_c.pt')\n",
    "test_labels_1=torch.load('test_labels_50_mut_1_c.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_words=len(train_data_1)\n",
    "num_test_words=len(test_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_words=48042 # 1000 words 50 misspilled single mistake \n",
    "num_test_words=5338 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recurrent_Layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(Recurrent_Layer, self).__init__()\n",
    "        \n",
    "\n",
    "        self.layer1 = nn.Embedding (vocab_size, hidden_size)\n",
    "        self.lstm=nn.LSTM( hidden_size, hidden_size, bidirectional=True)\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "        self.fc=nn.Linear( 2* hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, h_init,c_init):\n",
    "\n",
    "        g_seq          =self.layer1(x)\n",
    "        h_seq, (h_final, c_final) =self.lstm(g_seq, (h_init,c_init))\n",
    "        h_seq=self.dropout(h_seq)\n",
    "        score_seq      =self.fc(h_seq)\n",
    "        \n",
    "        return score_seq,  h_final , c_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recurrent_Layer(\n",
      "  (layer1): Embedding(26, 256)\n",
      "  (lstm): LSTM(256, 256, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.2)\n",
      "  (fc): Linear(in_features=512, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden_size=256\n",
    "vocab_size =26\n",
    "# word_length= 19 #  according to the data \n",
    "batch_of_words=1\n",
    "\n",
    "lstm_net=Recurrent_Layer(vocab_size,hidden_size).cuda()\n",
    "\n",
    "lstm_net = lstm_net.to(device)\n",
    "print(lstm_net)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "my_lr=0.05\n",
    "accu_plt=[]\n",
    "loss_plt=[]\n",
    "test_loss_plt=[]\n",
    "accuracy_test_plt=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error( scores , labels ):\n",
    "\n",
    "    bs=scores.size(0)\n",
    "    predicted_labels = scores.argmax(dim=1)\n",
    "    indicator = (predicted_labels == labels)\n",
    "    num_matches=indicator.sum()\n",
    "    corr=num_matches/len(predicted_labels)\n",
    "    return corr.item()\n",
    "\n",
    "def save_model_parameters():\n",
    "# save models \n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'lstm_model':lstm_net.state_dict(),\n",
    "                'lstm_optimizer': lstm_optimizer.state_dict(),\n",
    "                'train_loss': loss_plt, 'train_accuracy': accu_plt, \n",
    "                'test_loss': test_loss_plt,\n",
    "                'test_accuracy': accuracy_test_plt, 'confusion matrix_parameters':confusion_mtx_parameters  }, 'lstm_single_char_50_mutation_bid_drop_02.pt')\n",
    "#                 'incorrect_to_correct':confusion_mtx_parameters[0],  'correct_to_correct':confusion_mtx_parameters[2],\n",
    "#                 'correct_to_incorrect':confusion_mtx_parameters[1],'regenerate':confusion_mtx_parameters[3]\n",
    "  \n",
    "           \n",
    "               \n",
    "     \n",
    "        #print(in_word, out_word)\n",
    "        \n",
    "def confusion_parameters(scores,target_tensor,inpute_tensor,conf_counter):\n",
    "\n",
    "    \n",
    "    if torch.all(torch.eq(target_tensor, scores.argmax(dim=1)))==1 and torch.all(torch.eq(inpute_tensor,target_tensor))==0: ### making incorrect->correct\n",
    "        conf_counter[0] +=1\n",
    "\n",
    "    if torch.all(torch.eq(inpute_tensor, scores.argmax(dim=1)))==0 and torch.all(torch.eq(inpute_tensor,target_tensor))==1: ### making correct->incorrect\n",
    "\n",
    "        conf_counter[1] +=1    \n",
    "\n",
    "\n",
    "    if torch.all(torch.eq(inpute_tensor, scores.argmax(dim=1)))==1 and torch.all(torch.eq(inpute_tensor,target_tensor))==1: ### making correct->correct\n",
    "\n",
    "        conf_counter[2] +=1                                                               \n",
    "\n",
    "    if torch.all(torch.eq(inpute_tensor, scores.argmax(dim=1)))==1 and torch.all(torch.eq(inpute_tensor,target_tensor))==0: ### making inccorrect->correct or regenerate the input \n",
    "\n",
    "        conf_counter[3] +=1  \n",
    "    return conf_counter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "    # to deactivate dropout regularization during testing\n",
    "    lstm_net.eval()\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "    num_matches=0 \n",
    "    acc_plt=[]\n",
    "    loss_plt=[]\n",
    "    \n",
    "    # counts for computing correct and incorrect proportion\n",
    "    count=torch.zeros(4)\n",
    "    cnt_list=torch.zeros(4)\n",
    "    cnf_mtx_count=torch.zeros(4)\n",
    "    \n",
    "    for i in range(0,num_test_words):\n",
    "        \n",
    "        h = torch.zeros(2,batch_of_words,  hidden_size).cuda()\n",
    "        c = torch.zeros(2,batch_of_words,  hidden_size).cuda()\n",
    "        h=h.to(device)\n",
    "        c=c.to(device)\n",
    "        word_length= len(test_data_1 [i])\n",
    "        \n",
    "        inpute_tensor=  test_data_1[i].cuda()\n",
    "        target_tensor =  test_labels_1[i].cuda()\n",
    "        \n",
    "        # sending to GPU\n",
    "        inpute_tensor=inpute_tensor.to(device)\n",
    "        target_tensor=target_tensor.to(device)\n",
    "        # forward pass\n",
    "        scores_char, h, c= lstm_net(inpute_tensor.view(word_length,1), h, c)\n",
    "        # reshape before calculating the loss for easier slicing of mini batch of words\n",
    "        scores_char = scores_char.view(word_length*batch_of_words,vocab_size)\n",
    "        target_tensor = target_tensor.contiguous()\n",
    "        target_tensor = target_tensor.view(word_length*batch_of_words)\n",
    "        # calculating the loss of batch of character sthat constrcut a words\n",
    "        loss_char= criterion(scores_char, target_tensor)# do we need to add the loss for every \n",
    "        \n",
    "            #================================================#\n",
    "        # accumalate the loss\n",
    "        running_loss+= loss_char.item()\n",
    "        num_batches+=1 \n",
    "        # computing accuracy\n",
    "        num_matches+= get_error(scores_char, target_tensor)\n",
    "        #=======================================================#\n",
    "        # compute confusion paramters \n",
    "        cnf_mtx_count+=confusion_parameters(scores_char,target_tensor,inpute_tensor,count)\n",
    "        #============================================# \n",
    "        \n",
    "\n",
    "    ###====Different Metrics for evaluation=============###\n",
    "    \n",
    "    # counter for computing confusion matrix\n",
    "#     cnt_list[0]+=cnf_mtx_count[0].item()\n",
    "#     cnt_list[1]+=cnf_mtx_count[1].item()\n",
    "#     cnt_list[2]+=cnf_mtx_count[2].item()\n",
    "#     cnt_list[3]+=cnf_mtx_count[3].item()\n",
    "  \n",
    "    # accuracy and loss \n",
    "    accuracy= (num_matches/num_test_words)*100\n",
    "    acc_plt.append(accuracy)\n",
    "    total_loss = running_loss/num_batches \n",
    "    loss_plt.append(total_loss)\n",
    "    \n",
    "    # printing results \n",
    "    print('Test==: loss = ',(total_loss),'\\t accuracy=', accuracy,'%' )\n",
    "    return acc_plt, loss_plt, cnf_mtx_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test==: loss =  0.6082124173641205 \t accuracy= 20.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([20.0], [0.6082124173641205], tensor([9., 1., 6., 0.]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_on_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train::: epoch= 1 \t lr= 0.05 \t (loss)= 0.18853896301434886 \t (accuracy)= 64.87032180175679 % \t time= 164.76657223701477\n",
      "Test==: loss =  1.3834352731935906 \t accuracy= 2.660172349194455 %\n",
      "\n",
      "Train::: epoch= 2 \t lr= 0.05 \t (loss)= 0.1304543384860051 \t (accuracy)= 74.77831897090046 % \t time= 341.44678688049316\n",
      "Test==: loss =  1.2325140768421163 \t accuracy= 4.664668415136755 %\n",
      "\n",
      "Train::: epoch= 3 \t lr= 0.05 \t (loss)= 0.11625305223941712 \t (accuracy)= 77.24282919112443 % \t time= 516.7183020114899\n",
      "Test==: loss =  1.1888244432584683 \t accuracy= 4.777070063694268 %\n",
      "\n",
      "Train::: epoch= 4 \t lr= 0.05 \t (loss)= 0.10650655914574439 \t (accuracy)= 78.71862120644437 % \t time= 692.4947218894958\n",
      "Test==: loss =  1.1607471884599676 \t accuracy= 5.545147995503934 %\n",
      "\n",
      "Train::: epoch= 5 \t lr= 0.05 \t (loss)= 0.0987752673410938 \t (accuracy)= 80.17359810166104 % \t time= 867.3575296401978\n",
      "Test==: loss =  1.0624884485962 \t accuracy= 7.680779318096666 %\n",
      "\n",
      "Train::: epoch= 6 \t lr= 0.05 \t (loss)= 0.09143425864279349 \t (accuracy)= 81.50992881228925 % \t time= 1043.302011013031\n",
      "Test==: loss =  1.0793673893114264 \t accuracy= 7.306107156238291 %\n",
      "\n",
      "Train::: epoch= 7 \t lr= 0.05 \t (loss)= 0.08575941419864655 \t (accuracy)= 82.41954956080096 % \t time= 1217.9647188186646\n",
      "Test==: loss =  1.0292399320304813 \t accuracy= 8.317721993255901 %\n",
      "\n",
      "Train::: epoch= 8 \t lr= 0.05 \t (loss)= 0.08055709702920952 \t (accuracy)= 83.14391574039382 % \t time= 1393.7805488109589\n",
      "Test==: loss =  0.9051633207843595 \t accuracy= 10.809291869614087 %\n",
      "\n",
      "Train::: epoch= 9 \t lr= 0.05 \t (loss)= 0.07495903651077947 \t (accuracy)= 84.06394404895717 % \t time= 1569.8202095031738\n",
      "Test==: loss =  0.8789144244922076 \t accuracy= 13.150992881228923 %\n",
      "\n",
      "Train::: epoch= 10 \t lr= 0.045454545454545456 \t (loss)= 0.07085523528787588 \t (accuracy)= 84.53852878731111 % \t time= 1745.8843088150024\n",
      "Test==: loss =  0.9170159996508604 \t accuracy= 13.713001124016486 %\n",
      "\n",
      "Train::: epoch= 11 \t lr= 0.045454545454545456 \t (loss)= 0.06390697284501286 \t (accuracy)= 85.56471420840099 % \t time= 1921.3955481052399\n",
      "Test==: loss =  0.7332110056587107 \t accuracy= 18.84600974147621 %\n",
      "\n",
      "Train::: epoch= 12 \t lr= 0.045454545454545456 \t (loss)= 0.05960088550509749 \t (accuracy)= 86.5846550934599 % \t time= 2096.7897050380707\n",
      "Test==: loss =  0.7827852308687095 \t accuracy= 17.703259647808167 %\n",
      "\n",
      "Train::: epoch= 13 \t lr= 0.045454545454545456 \t (loss)= 0.0558221520273939 \t (accuracy)= 87.2195162566088 % \t time= 2271.9654676914215\n",
      "Test==: loss =  0.7186393873168765 \t accuracy= 19.67028849756463 %\n",
      "\n",
      "Train::: epoch= 14 \t lr= 0.045454545454545456 \t (loss)= 0.05174020858178269 \t (accuracy)= 88.00216477249073 % \t time= 2446.6699934005737\n",
      "Test==: loss =  0.7313459682360721 \t accuracy= 19.632821281378792 %\n",
      "\n",
      "Train::: epoch= 15 \t lr= 0.045454545454545456 \t (loss)= 0.048848143648642726 \t (accuracy)= 88.6245368635777 % \t time= 2623.063333272934\n",
      "Test==: loss =  0.7310242685882966 \t accuracy= 21.768452603971525 %\n",
      "\n",
      "Train::: epoch= 16 \t lr= 0.045454545454545456 \t (loss)= 0.04512322047264325 \t (accuracy)= 89.34057699512927 % \t time= 2799.203607082367\n",
      "Test==: loss =  0.7030251010135105 \t accuracy= 23.86661671037842 %\n",
      "\n",
      "Train::: epoch= 17 \t lr= 0.045454545454545456 \t (loss)= 0.041635606441248973 \t (accuracy)= 90.06910619874276 % \t time= 2975.086547613144\n",
      "Test==: loss =  0.730383698724891 \t accuracy= 22.98613713001124 %\n",
      "\n",
      "Train::: epoch= 18 \t lr= 0.045454545454545456 \t (loss)= 0.04016392571271431 \t (accuracy)= 90.29182798384745 % \t time= 3150.0066316127777\n",
      "Test==: loss =  0.6799106306908492 \t accuracy= 25.44023979018359 %\n",
      "\n",
      "Train::: epoch= 19 \t lr= 0.045454545454545456 \t (loss)= 0.03774212682939811 \t (accuracy)= 90.96207485117189 % \t time= 3324.957790374756\n",
      "Test==: loss =  0.7049102778420689 \t accuracy= 24.840764331210192 %\n",
      "\n",
      "Train::: epoch= 20 \t lr= 0.04132231404958678 \t (loss)= 0.03536210706816079 \t (accuracy)= 91.50743099787687 % \t time= 3500.2118515968323\n",
      "Test==: loss =  0.6459281367632673 \t accuracy= 30.048707381041588 %\n",
      "\n",
      "Train::: epoch= 21 \t lr= 0.04132231404958678 \t (loss)= 0.033060393564099 \t (accuracy)= 91.8800216477249 % \t time= 3675.195602416992\n",
      "Test==: loss =  0.6552609012155599 \t accuracy= 31.997002622705136 %\n",
      "\n",
      "Train::: epoch= 22 \t lr= 0.04132231404958678 \t (loss)= 0.03106421685781663 \t (accuracy)= 92.19849298530453 % \t time= 3850.713511943817\n",
      "Test==: loss =  0.6241845556170352 \t accuracy= 31.959535406519297 %\n",
      "\n",
      "Train::: epoch= 23 \t lr= 0.04132231404958678 \t (loss)= 0.029383638528508723 \t (accuracy)= 92.71262645185463 % \t time= 4026.37607216835\n",
      "Test==: loss =  0.6572246497710637 \t accuracy= 31.715998501311354 %\n",
      "\n",
      "Train::: epoch= 24 \t lr= 0.04132231404958678 \t (loss)= 0.02823241406635285 \t (accuracy)= 93.05399442154781 % \t time= 4201.846822977066\n",
      "Test==: loss =  0.5985340717964512 \t accuracy= 35.91232671412514 %\n",
      "\n",
      "Train::: epoch= 25 \t lr= 0.04132231404958678 \t (loss)= 0.026832613625434987 \t (accuracy)= 93.36413971108614 % \t time= 4377.073709964752\n",
      "Test==: loss =  0.7125077891443333 \t accuracy= 32.05320344698389 %\n",
      "\n",
      "Train::: epoch= 26 \t lr= 0.04132231404958678 \t (loss)= 0.025538018342665923 \t (accuracy)= 93.54523125598435 % \t time= 4552.493903875351\n",
      "Test==: loss =  0.6489244875264134 \t accuracy= 34.24503559385538 %\n",
      "\n",
      "Train::: epoch= 27 \t lr= 0.04132231404958678 \t (loss)= 0.024624316813586275 \t (accuracy)= 93.87202864160527 % \t time= 4728.070953130722\n",
      "Test==: loss =  0.6671445594303739 \t accuracy= 32.61521168977145 %\n",
      "\n",
      "Train::: epoch= 28 \t lr= 0.04132231404958678 \t (loss)= 0.02346673958050165 \t (accuracy)= 94.21755963531909 % \t time= 4903.803952455521\n",
      "Test==: loss =  0.6351985321324262 \t accuracy= 33.92656425627576 %\n",
      "\n",
      "Train::: epoch= 29 \t lr= 0.04132231404958678 \t (loss)= 0.022450430059014093 \t (accuracy)= 94.36534698805212 % \t time= 5079.425062656403\n",
      "Test==: loss =  0.6300792929859467 \t accuracy= 37.24241288872237 %\n",
      "\n",
      "Train::: epoch= 30 \t lr= 0.037565740045078885 \t (loss)= 0.022076351967184462 \t (accuracy)= 94.4923192206819 % \t time= 5254.744658946991\n",
      "Test==: loss =  0.6176168184237996 \t accuracy= 37.20494567253653 %\n",
      "\n",
      "Train::: epoch= 31 \t lr= 0.037565740045078885 \t (loss)= 0.02067504046332717 \t (accuracy)= 94.83160567836477 % \t time= 5430.333044290543\n",
      "Test==: loss =  0.6065686723164337 \t accuracy= 38.87223679280629 %\n",
      "\n",
      "Train::: epoch= 32 \t lr= 0.037565740045078885 \t (loss)= 0.019712665379087675 \t (accuracy)= 95.06473502352108 % \t time= 5604.886939525604\n",
      "Test==: loss =  0.6258354740042315 \t accuracy= 39.52791307605845 %\n",
      "\n",
      "Train::: epoch= 33 \t lr= 0.037565740045078885 \t (loss)= 0.018773110392521462 \t (accuracy)= 95.27496773656384 % \t time= 5780.343546628952\n",
      "Test==: loss =  0.5569197189693361 \t accuracy= 42.20681903334582 %\n",
      "\n",
      "Train::: epoch= 34 \t lr= 0.037565740045078885 \t (loss)= 0.018639644212699518 \t (accuracy)= 95.43108113733815 % \t time= 5955.837800502777\n",
      "Test==: loss =  0.554692100046605 \t accuracy= 42.131884600974146 %\n",
      "\n",
      "Train::: epoch= 35 \t lr= 0.037565740045078885 \t (loss)= 0.017846372577409203 \t (accuracy)= 95.63923233837059 % \t time= 6131.123919963837\n",
      "Test==: loss =  0.5461126953954675 \t accuracy= 43.06856500562008 %\n",
      "\n",
      "Train::: epoch= 36 \t lr= 0.037565740045078885 \t (loss)= 0.01706918871059029 \t (accuracy)= 95.70375921069065 % \t time= 6307.046238183975\n",
      "Test==: loss =  0.559684837156873 \t accuracy= 43.16223304608467 %\n",
      "\n",
      "Train::: epoch= 37 \t lr= 0.037565740045078885 \t (loss)= 0.01674827845731329 \t (accuracy)= 95.92648099579534 % \t time= 6481.443341255188\n",
      "Test==: loss =  0.5468598215009526 \t accuracy= 45.33533158486324 %\n",
      "\n",
      "Train::: epoch= 38 \t lr= 0.037565740045078885 \t (loss)= 0.017364730364906378 \t (accuracy)= 95.84322051538237 % \t time= 6655.375687122345\n",
      "Test==: loss =  0.5320933496970954 \t accuracy= 44.192581491195206 %\n",
      "\n",
      "Train::: epoch= 39 \t lr= 0.037565740045078885 \t (loss)= 0.01619249597102488 \t (accuracy)= 96.04929020440449 % \t time= 6829.683876514435\n",
      "Test==: loss =  0.47886275323784394 \t accuracy= 47.452229299363054 %\n",
      "\n",
      "Train::: epoch= 40 \t lr= 0.03415067276825353 \t (loss)= 0.016145356117425787 \t (accuracy)= 96.0930019566213 % \t time= 7004.71385550499\n",
      "Test==: loss =  0.5286059982266784 \t accuracy= 45.447733233420756 %\n",
      "\n",
      "Train::: epoch= 41 \t lr= 0.03415067276825353 \t (loss)= 0.015297459543548142 \t (accuracy)= 96.20956662919944 % \t time= 7178.998071670532\n",
      "Test==: loss =  0.45361200411072206 \t accuracy= 49.17572124391158 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train::: epoch= 42 \t lr= 0.03415067276825353 \t (loss)= 0.015157261144410662 \t (accuracy)= 96.37608759002539 % \t time= 7355.519688844681\n",
      "Test==: loss =  0.46278255459782386 \t accuracy= 49.11952041963282 %\n",
      "\n",
      "Train::: epoch= 43 \t lr= 0.03415067276825353 \t (loss)= 0.014527000389397794 \t (accuracy)= 96.40731027018026 % \t time= 7529.79181599617\n",
      "Test==: loss =  0.4189266533096916 \t accuracy= 51.04908205320344 %\n",
      "\n",
      "Train::: epoch= 44 \t lr= 0.03415067276825353 \t (loss)= 0.014524065967371597 \t (accuracy)= 96.44477748636609 % \t time= 7703.372896671295\n",
      "Test==: loss =  0.40271216578480595 \t accuracy= 52.43536905207943 %\n",
      "\n",
      "Train::: epoch= 45 \t lr= 0.03415067276825353 \t (loss)= 0.013754167504134275 \t (accuracy)= 96.63835810332625 % \t time= 7878.5569949150085\n",
      "Test==: loss =  0.37879656203729334 \t accuracy= 54.75833645560135 %\n",
      "\n",
      "Train::: epoch= 46 \t lr= 0.03415067276825353 \t (loss)= 0.013491409904441158 \t (accuracy)= 96.83402023229674 % \t time= 8052.830947160721\n",
      "Test==: loss =  0.37004305951247035 \t accuracy= 55.976020981641064 %\n",
      "\n",
      "Train::: epoch= 47 \t lr= 0.03415067276825353 \t (loss)= 0.013319525425725079 \t (accuracy)= 96.78198243203863 % \t time= 8227.070450305939\n",
      "Test==: loss =  0.3637484339302118 \t accuracy= 55.807418508804794 %\n",
      "\n",
      "Train::: epoch= 48 \t lr= 0.03415067276825353 \t (loss)= 0.01346852491443723 \t (accuracy)= 96.78198243203863 % \t time= 8401.99398136139\n",
      "Test==: loss =  0.32620460813495367 \t accuracy= 59.647808167853135 %\n",
      "\n",
      "Train::: epoch= 49 \t lr= 0.03415067276825353 \t (loss)= 0.01297076534138387 \t (accuracy)= 96.87981349652387 % \t time= 8576.16625905037\n",
      "Test==: loss =  0.3280953970034579 \t accuracy= 58.87973023604346 %\n",
      "\n",
      "Train::: epoch= 50 \t lr= 0.03104606615295775 \t (loss)= 0.012716733312330716 \t (accuracy)= 97.03384538528788 % \t time= 8750.563125133514\n",
      "Test==: loss =  0.32184213458946787 \t accuracy= 59.29186961408768 %\n",
      "\n",
      "Train::: epoch= 51 \t lr= 0.03104606615295775 \t (loss)= 0.012543701416603397 \t (accuracy)= 97.05882352941177 % \t time= 8924.430443763733\n",
      "Test==: loss =  0.31318586057239917 \t accuracy= 60.88422630198577 %\n",
      "\n",
      "Train::: epoch= 52 \t lr= 0.03104606615295775 \t (loss)= 0.012161700202724855 \t (accuracy)= 97.156654593897 % \t time= 9099.716140985489\n",
      "Test==: loss =  0.2911486335608899 \t accuracy= 62.532783814162606 %\n",
      "\n",
      "Train::: epoch= 53 \t lr= 0.03104606615295775 \t (loss)= 0.01187741135932543 \t (accuracy)= 97.1649806419383 % \t time= 9274.942440509796\n",
      "Test==: loss =  0.28367262801764304 \t accuracy= 63.15099288122893 %\n",
      "\n",
      "Train::: epoch= 54 \t lr= 0.03104606615295775 \t (loss)= 0.011648582865146515 \t (accuracy)= 97.30652345864036 % \t time= 9449.17022562027\n",
      "Test==: loss =  0.2840300372611262 \t accuracy= 63.5256650430873 %\n",
      "\n",
      "Train::: epoch= 55 \t lr= 0.03104606615295775 \t (loss)= 0.011674568572439929 \t (accuracy)= 97.2669747304442 % \t time= 9622.80719590187\n",
      "Test==: loss =  0.265389688253024 \t accuracy= 65.00562008242787 %\n",
      "\n",
      "Train::: epoch= 56 \t lr= 0.03104606615295775 \t (loss)= 0.011105495237297707 \t (accuracy)= 97.4563923233837 % \t time= 9798.437688112259\n",
      "Test==: loss =  0.25165883589801535 \t accuracy= 65.49269389284376 %\n",
      "\n",
      "Train::: epoch= 57 \t lr= 0.03104606615295775 \t (loss)= 0.011335632616552014 \t (accuracy)= 97.35439823487782 % \t time= 9972.703854322433\n",
      "Test==: loss =  0.23006947149229895 \t accuracy= 69.29561633570626 %\n",
      "\n",
      "Train::: epoch= 58 \t lr= 0.03104606615295775 \t (loss)= 0.01098296822549969 \t (accuracy)= 97.45222929936305 % \t time= 10148.347879171371\n",
      "Test==: loss =  0.22857678713343507 \t accuracy= 69.27688272761334 %\n",
      "\n",
      "Train::: epoch= 59 \t lr= 0.03104606615295775 \t (loss)= 0.011293509594619347 \t (accuracy)= 97.47928895549728 % \t time= 10323.452986955643\n",
      "Test==: loss =  0.21788104147815848 \t accuracy= 70.53203446983889 %\n",
      "\n",
      "Train::: epoch= 60 \t lr= 0.028223696502688862 \t (loss)= 0.01094182670165246 \t (accuracy)= 97.4896965155489 % \t time= 10497.796939373016\n",
      "Test==: loss =  0.1978566453113301 \t accuracy= 72.49906331959536 %\n",
      "\n",
      "Train::: epoch= 61 \t lr= 0.028223696502688862 \t (loss)= 0.010362799220073502 \t (accuracy)= 97.61666874817868 % \t time= 10671.994609355927\n",
      "Test==: loss =  0.20083004090266154 \t accuracy= 71.74971899587861 %\n",
      "\n",
      "Train::: epoch= 62 \t lr= 0.028223696502688862 \t (loss)= 0.010312794821762228 \t (accuracy)= 97.62291328420964 % \t time= 10844.398672103882\n",
      "Test==: loss =  0.1787846176712782 \t accuracy= 74.5597602098164 %\n",
      "\n",
      "Train::: epoch= 63 \t lr= 0.028223696502688862 \t (loss)= 0.010357264041022956 \t (accuracy)= 97.62915782024062 % \t time= 11019.480235815048\n",
      "Test==: loss =  0.18920874905654325 \t accuracy= 73.49194454852005 %\n",
      "\n",
      "Train::: epoch= 64 \t lr= 0.028223696502688862 \t (loss)= 0.00987493548161248 \t (accuracy)= 97.74572249281879 % \t time= 11192.494053125381\n",
      "Test==: loss =  0.1823188543474326 \t accuracy= 74.40989134507306 %\n",
      "\n",
      "Train::: epoch= 65 \t lr= 0.028223696502688862 \t (loss)= 0.009925673265952643 \t (accuracy)= 97.79984180508721 % \t time= 11368.485546588898\n",
      "Test==: loss =  0.1922531501822973 \t accuracy= 73.62307980517048 %\n",
      "\n",
      "Train::: epoch= 66 \t lr= 0.028223696502688862 \t (loss)= 0.00986419741259741 \t (accuracy)= 97.7478040048291 % \t time= 11543.369967222214\n",
      "Test==: loss =  0.17703103228428904 \t accuracy= 75.73997751967029 %\n",
      "\n",
      "Train::: epoch= 67 \t lr= 0.028223696502688862 \t (loss)= 0.009820117475267488 \t (accuracy)= 97.75196702884975 % \t time= 11718.998920917511\n",
      "Test==: loss =  0.1712825727554751 \t accuracy= 75.59010865492694 %\n",
      "\n",
      "Train::: epoch= 68 \t lr= 0.028223696502688862 \t (loss)= 0.009452352005997606 \t (accuracy)= 97.93722159776863 % \t time= 11894.320224523544\n",
      "Test==: loss =  0.16718730038264634 \t accuracy= 76.18958411390034 %\n",
      "\n",
      "Train::: epoch= 69 \t lr= 0.028223696502688862 \t (loss)= 0.009553729643306563 \t (accuracy)= 97.8248199492111 % \t time= 12068.143642187119\n",
      "Test==: loss =  0.1583359624914178 \t accuracy= 77.53840389659048 %\n",
      "\n",
      "Train::: epoch= 70 \t lr= 0.025657905911535328 \t (loss)= 0.009076325927503017 \t (accuracy)= 97.89142833354148 % \t time= 12243.63966870308\n",
      "Test==: loss =  0.16170990036409438 \t accuracy= 77.74447358561258 %\n",
      "\n",
      "Train::: epoch= 71 \t lr= 0.025657905911535328 \t (loss)= 0.00914359842189787 \t (accuracy)= 97.84355355730403 % \t time= 12417.837913751602\n",
      "Test==: loss =  0.15800388839233093 \t accuracy= 77.70700636942675 %\n",
      "\n",
      "Train::: epoch= 72 \t lr= 0.025657905911535328 \t (loss)= 0.008958331304089196 \t (accuracy)= 97.98925939802673 % \t time= 12593.625629425049\n",
      "Test==: loss =  0.1636393125493298 \t accuracy= 78.00674409891344 %\n",
      "\n",
      "Train::: epoch= 73 \t lr= 0.025657905911535328 \t (loss)= 0.008651117815390115 \t (accuracy)= 98.06627534240873 % \t time= 12781.46831870079\n",
      "Test==: loss =  0.16746037600870417 \t accuracy= 77.72573997751967 %\n",
      "\n",
      "Train::: epoch= 74 \t lr= 0.025657905911535328 \t (loss)= 0.008983865424967692 \t (accuracy)= 97.99758544606803 % \t time= 13105.67998123169\n",
      "Test==: loss =  0.1598935952945773 \t accuracy= 78.60621955788685 %\n",
      "\n",
      "Train::: epoch= 75 \t lr= 0.025657905911535328 \t (loss)= 0.008449604632893536 \t (accuracy)= 98.157861870863 % \t time= 13429.363812685013\n",
      "Test==: loss =  0.15842415208388563 \t accuracy= 78.98089171974523 %\n",
      "\n",
      "Train::: epoch= 76 \t lr= 0.025657905911535328 \t (loss)= 0.00846635714861167 \t (accuracy)= 98.07043836642937 % \t time= 13753.460354566574\n",
      "Test==: loss =  0.16566610417552063 \t accuracy= 77.96927688272761 %\n",
      "\n",
      "Train::: epoch= 77 \t lr= 0.025657905911535328 \t (loss)= 0.00894781637612741 \t (accuracy)= 98.09749802256358 % \t time= 14078.224962711334\n",
      "Test==: loss =  0.15078370779760578 \t accuracy= 79.82390408392656 %\n",
      "\n",
      "Train::: epoch= 78 \t lr= 0.025657905911535328 \t (loss)= 0.008055883072243433 \t (accuracy)= 98.20365513509013 % \t time= 14400.668511867523\n",
      "Test==: loss =  0.1491661440490324 \t accuracy= 79.59910078681153 %\n",
      "\n",
      "Train::: epoch= 79 \t lr= 0.025657905911535328 \t (loss)= 0.008261279511380318 \t (accuracy)= 98.20781815911079 % \t time= 14724.886485815048\n",
      "Test==: loss =  0.14622132865023726 \t accuracy= 79.69276882727613 %\n",
      "\n",
      "Train::: epoch= 80 \t lr= 0.02332536901048666 \t (loss)= 0.008293306636295428 \t (accuracy)= 98.12455767869781 % \t time= 15048.254904270172\n",
      "Test==: loss =  0.13945197013119148 \t accuracy= 80.91045335331584 %\n",
      "\n",
      "Train::: epoch= 81 \t lr= 0.02332536901048666 \t (loss)= 0.00807052404333334 \t (accuracy)= 98.30773073560634 % \t time= 15372.912142753601\n",
      "Test==: loss =  0.1357550842212831 \t accuracy= 81.2663919070813 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train::: epoch= 82 \t lr= 0.02332536901048666 \t (loss)= 0.008027475286808663 \t (accuracy)= 98.16202489488364 % \t time= 15696.700811624527\n",
      "Test==: loss =  0.13611755429975733 \t accuracy= 81.50992881228925 %\n",
      "\n",
      "Train::: epoch= 83 \t lr= 0.02332536901048666 \t (loss)= 0.007905288109577158 \t (accuracy)= 98.23487781524499 % \t time= 16019.828091144562\n",
      "Test==: loss =  0.1342184443195652 \t accuracy= 81.62233046084675 %\n",
      "\n",
      "Train::: epoch= 84 \t lr= 0.02332536901048666 \t (loss)= 0.007761153387806281 \t (accuracy)= 98.29940468756504 % \t time= 16343.59430384636\n",
      "Test==: loss =  0.13410610233532294 \t accuracy= 81.11652304233795 %\n",
      "\n",
      "Train::: epoch= 85 \t lr= 0.02332536901048666 \t (loss)= 0.007589368049227663 \t (accuracy)= 98.38266516797802 % \t time= 16667.892414808273\n",
      "Test==: loss =  0.1330988277852082 \t accuracy= 81.97826901461221 %\n",
      "\n",
      "Train::: epoch= 86 \t lr= 0.02332536901048666 \t (loss)= 0.00768719894099721 \t (accuracy)= 98.24944839931726 % \t time= 16993.262807369232\n",
      "Test==: loss =  0.12739110775783657 \t accuracy= 82.12813787935556 %\n",
      "\n",
      "Train::: epoch= 87 \t lr= 0.02332536901048666 \t (loss)= 0.007916051421812217 \t (accuracy)= 98.31605678364764 % \t time= 17319.364814043045\n",
      "Test==: loss =  0.12751731593062912 \t accuracy= 82.4653428250281 %\n",
      "\n",
      "Train::: epoch= 88 \t lr= 0.02332536901048666 \t (loss)= 0.007790112221069914 \t (accuracy)= 98.30148619957538 % \t time= 17645.68494772911\n",
      "Test==: loss =  0.12413692063374784 \t accuracy= 82.87748220307232 %\n",
      "\n",
      "Train::: epoch= 89 \t lr= 0.02332536901048666 \t (loss)= 0.00833464365690155 \t (accuracy)= 98.13288372673911 % \t time= 17971.11369037628\n",
      "Test==: loss =  0.12467958837191913 \t accuracy= 82.37167478456351 %\n",
      "\n",
      "Train::: epoch= 90 \t lr= 0.021204880918624235 \t (loss)= 0.007709476570590594 \t (accuracy)= 98.34311643978185 % \t time= 18297.594643831253\n",
      "Test==: loss =  0.12080472133775665 \t accuracy= 83.10228550018734 %\n",
      "\n",
      "Train::: epoch= 91 \t lr= 0.021204880918624235 \t (loss)= 0.007630078314009819 \t (accuracy)= 98.33062736771991 % \t time= 18623.802934885025\n",
      "Test==: loss =  0.11965698863421796 \t accuracy= 82.91494941925815 %\n",
      "\n",
      "Train::: epoch= 92 \t lr= 0.021204880918624235 \t (loss)= 0.007463825158847133 \t (accuracy)= 98.42013238416386 % \t time= 18948.66030049324\n",
      "Test==: loss =  0.11963699587805102 \t accuracy= 83.60809291869614 %\n",
      "\n",
      "Train::: epoch= 93 \t lr= 0.021204880918624235 \t (loss)= 0.007449617107405168 \t (accuracy)= 98.39307272802964 % \t time= 19275.763719558716\n",
      "Test==: loss =  0.1185086301719508 \t accuracy= 83.79542899962533 %\n",
      "\n",
      "Train::: epoch= 94 \t lr= 0.021204880918624235 \t (loss)= 0.007499731840729506 \t (accuracy)= 98.36185004787478 % \t time= 19602.27429008484\n",
      "Test==: loss =  0.11164349381484423 \t accuracy= 84.41363806669165 %\n",
      "\n",
      "Train::: epoch= 95 \t lr= 0.021204880918624235 \t (loss)= 0.007228862256572439 \t (accuracy)= 98.42013238416386 % \t time= 19926.853736162186\n",
      "Test==: loss =  0.11252547819242692 \t accuracy= 84.13263394529787 %\n",
      "\n",
      "Train::: epoch= 96 \t lr= 0.021204880918624235 \t (loss)= 0.007363655533728216 \t (accuracy)= 98.39931726406061 % \t time= 20252.586520433426\n",
      "Test==: loss =  0.11030159788137134 \t accuracy= 84.50730610715624 %\n",
      "\n",
      "Train::: epoch= 97 \t lr= 0.021204880918624235 \t (loss)= 0.007623686031049944 \t (accuracy)= 98.41805087215353 % \t time= 20578.40528488159\n",
      "Test==: loss =  0.11069253845197692 \t accuracy= 84.67590857999251 %\n",
      "\n",
      "Train::: epoch= 98 \t lr= 0.021204880918624235 \t (loss)= 0.007209193123831348 \t (accuracy)= 98.41805087215353 % \t time= 20906.10458779335\n",
      "Test==: loss =  0.10760843155447608 \t accuracy= 84.9756463094792 %\n",
      "\n",
      "Train::: epoch= 99 \t lr= 0.021204880918624235 \t (loss)= 0.007365751345488936 \t (accuracy)= 98.38474667998834 % \t time= 21231.14378285408\n",
      "Test==: loss =  0.10544308331848397 \t accuracy= 85.4627201198951 %\n",
      "\n",
      "Train::: epoch= 100 \t lr= 0.019277164471476576 \t (loss)= 0.007413562164898882 \t (accuracy)= 98.37642063194704 % \t time= 21557.479479551315\n",
      "Test==: loss =  0.1041229861949582 \t accuracy= 85.35031847133759 %\n",
      "\n",
      "Train::: epoch= 101 \t lr= 0.019277164471476576 \t (loss)= 0.007545785827632858 \t (accuracy)= 98.32854585570959 % \t time= 21884.522218227386\n",
      "Test==: loss =  0.10214757586926626 \t accuracy= 85.55638816035969 %\n",
      "\n",
      "Train::: epoch= 102 \t lr= 0.019277164471476576 \t (loss)= 0.006904502986654151 \t (accuracy)= 98.48674076849423 % \t time= 22211.051971673965\n",
      "Test==: loss =  0.09773207241285756 \t accuracy= 85.96852753840389 %\n",
      "\n",
      "Train::: epoch= 103 \t lr= 0.019277164471476576 \t (loss)= 0.0071493004760089 \t (accuracy)= 98.51380042462846 % \t time= 22537.268290042877\n",
      "Test==: loss =  0.09755362659461736 \t accuracy= 86.17459722742599 %\n",
      "\n",
      "Train::: epoch= 104 \t lr= 0.019277164471476576 \t (loss)= 0.006858083881475584 \t (accuracy)= 98.54502310478331 % \t time= 22862.145333766937\n",
      "Test==: loss =  0.09724552362945138 \t accuracy= 85.79992506556763 %\n",
      "\n",
      "Train::: epoch= 105 \t lr= 0.019277164471476576 \t (loss)= 0.006926849250914836 \t (accuracy)= 98.49922984055618 % \t time= 23187.199288129807\n",
      "Test==: loss =  0.09520475899523861 \t accuracy= 86.19333083551892 %\n",
      "\n",
      "Train::: epoch= 106 \t lr= 0.019277164471476576 \t (loss)= 0.00678897885040546 \t (accuracy)= 98.54918612880397 % \t time= 23514.116975069046\n",
      "Test==: loss =  0.09396989531038101 \t accuracy= 86.54926938928438 %\n",
      "\n",
      "Train::: epoch= 107 \t lr= 0.019277164471476576 \t (loss)= 0.006619339771803008 \t (accuracy)= 98.54294159277299 % \t time= 23838.15328860283\n",
      "Test==: loss =  0.09311044596883 \t accuracy= 86.36193330835519 %\n",
      "\n",
      "Train::: epoch= 108 \t lr= 0.019277164471476576 \t (loss)= 0.006995789585546372 \t (accuracy)= 98.51588193663878 % \t time= 24162.2671251297\n",
      "Test==: loss =  0.09078168793218162 \t accuracy= 86.75533907830648 %\n",
      "\n",
      "Train::: epoch= 109 \t lr= 0.019277164471476576 \t (loss)= 0.0069041262125106315 \t (accuracy)= 98.46592564839099 % \t time= 24487.402891397476\n",
      "Test==: loss =  0.08930943577378168 \t accuracy= 87.18621206444361 %\n",
      "\n",
      "Train::: epoch= 110 \t lr= 0.017524694974069614 \t (loss)= 0.006784649649135523 \t (accuracy)= 98.5616752008659 % \t time= 24813.238970518112\n",
      "Test==: loss =  0.08814314804196305 \t accuracy= 87.09254402397903 %\n",
      "\n",
      "Train::: epoch= 111 \t lr= 0.017524694974069614 \t (loss)= 0.006728087038197378 \t (accuracy)= 98.55126764081429 % \t time= 25139.33860349655\n",
      "Test==: loss =  0.08604152288328067 \t accuracy= 87.11127763207193 %\n",
      "\n",
      "Train::: epoch= 112 \t lr= 0.017524694974069614 \t (loss)= 0.006855098401404487 \t (accuracy)= 98.59289788102078 % \t time= 25464.218687534332\n",
      "Test==: loss =  0.08606765878408702 \t accuracy= 87.2798801049082 %\n",
      "\n",
      "Train::: epoch= 113 \t lr= 0.017524694974069614 \t (loss)= 0.006946975075317305 \t (accuracy)= 98.52420798468006 % \t time= 25790.00224351883\n",
      "Test==: loss =  0.08376556199339384 \t accuracy= 87.41101536155864 %\n",
      "\n",
      "Train::: epoch= 114 \t lr= 0.017524694974069614 \t (loss)= 0.006719561608925851 \t (accuracy)= 98.59081636901045 % \t time= 26116.048259973526\n",
      "Test==: loss =  0.08186962513928266 \t accuracy= 88.06669164481079 %\n",
      "\n",
      "Train::: epoch= 115 \t lr= 0.017524694974069614 \t (loss)= 0.006703293443963977 \t (accuracy)= 98.6386911452479 % \t time= 26442.112869262695\n",
      "Test==: loss =  0.08317066641372638 \t accuracy= 87.59835144248783 %\n",
      "\n",
      "Train::: epoch= 116 \t lr= 0.017524694974069614 \t (loss)= 0.006825175328950338 \t (accuracy)= 98.65326172932018 % \t time= 26767.300762176514\n",
      "Test==: loss =  0.08177949213582575 \t accuracy= 88.06669164481079 %\n",
      "\n",
      "Train::: epoch= 117 \t lr= 0.017524694974069614 \t (loss)= 0.006485998308538818 \t (accuracy)= 98.60538695308271 % \t time= 27094.660150527954\n",
      "Test==: loss =  0.08034080285610702 \t accuracy= 88.14162607718247 %\n",
      "\n",
      "Train::: epoch= 118 \t lr= 0.017524694974069614 \t (loss)= 0.006576998488095061 \t (accuracy)= 98.61995753715499 % \t time= 27421.086716413498\n",
      "Test==: loss =  0.08051487812970475 \t accuracy= 88.0854252529037 %\n",
      "\n",
      "Train::: epoch= 119 \t lr= 0.017524694974069614 \t (loss)= 0.006510502648003697 \t (accuracy)= 98.64909870529954 % \t time= 27748.007302045822\n",
      "Test==: loss =  0.07984074487553743 \t accuracy= 88.40389659048333 %\n",
      "\n",
      "Train::: epoch= 120 \t lr= 0.01593154088551783 \t (loss)= 0.006607285404908193 \t (accuracy)= 98.66158777736148 % \t time= 28075.393379449844\n",
      "Test==: loss =  0.07884508024920088 \t accuracy= 88.32896215811165 %\n",
      "\n",
      "Train::: epoch= 121 \t lr= 0.01593154088551783 \t (loss)= 0.006572489375983731 \t (accuracy)= 98.64077265725824 % \t time= 28399.6274497509\n",
      "Test==: loss =  0.07755645738703927 \t accuracy= 88.75983514424878 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train::: epoch= 122 \t lr= 0.01593154088551783 \t (loss)= 0.006396772765662182 \t (accuracy)= 98.70946255359894 % \t time= 28724.955037117004\n",
      "Test==: loss =  0.07882265573962266 \t accuracy= 88.40389659048333 %\n",
      "\n",
      "Train::: epoch= 123 \t lr= 0.01593154088551783 \t (loss)= 0.006532235842934829 \t (accuracy)= 98.65950626535115 % \t time= 29049.8575463295\n",
      "Test==: loss =  0.07888742699900195 \t accuracy= 88.51629823904084 %\n",
      "\n",
      "Train::: epoch= 124 \t lr= 0.01593154088551783 \t (loss)= 0.006323787176003763 \t (accuracy)= 98.6990549935473 % \t time= 29375.160282611847\n",
      "Test==: loss =  0.07689629879738616 \t accuracy= 88.81603596852754 %\n",
      "\n",
      "Train::: epoch= 125 \t lr= 0.01593154088551783 \t (loss)= 0.006188036723690964 \t (accuracy)= 98.75109279380541 % \t time= 29702.017313957214\n",
      "Test==: loss =  0.07757745238801836 \t accuracy= 88.40389659048333 %\n",
      "\n",
      "Train::: epoch= 126 \t lr= 0.01593154088551783 \t (loss)= 0.006022611497881458 \t (accuracy)= 98.80729361808417 % \t time= 30028.611914634705\n",
      "Test==: loss =  0.07692535582635802 \t accuracy= 88.6849007118771 %\n",
      "\n",
      "Train::: epoch= 127 \t lr= 0.01593154088551783 \t (loss)= 0.005957424121821475 \t (accuracy)= 98.78856000999126 % \t time= 30353.076323270798\n",
      "Test==: loss =  0.07778360360964362 \t accuracy= 88.81603596852754 %\n",
      "\n",
      "Train::: epoch= 128 \t lr= 0.01593154088551783 \t (loss)= 0.006027986457125083 \t (accuracy)= 98.70529952957828 % \t time= 30678.15363097191\n",
      "Test==: loss =  0.07615341727657043 \t accuracy= 88.75983514424878 %\n",
      "\n",
      "Train::: epoch= 129 \t lr= 0.01593154088551783 \t (loss)= 0.005850560934427445 \t (accuracy)= 98.79896757004289 % \t time= 31005.197612524033\n",
      "Test==: loss =  0.07495906698849307 \t accuracy= 89.13450730610715 %\n",
      "\n",
      "Train::: epoch= 130 \t lr= 0.01448321898683439 \t (loss)= 0.005856147107666537 \t (accuracy)= 98.79480454602223 % \t time= 31331.4320499897\n",
      "Test==: loss =  0.0743818274304074 \t accuracy= 89.2843761708505 %\n",
      "\n",
      "Train::: epoch= 131 \t lr= 0.01448321898683439 \t (loss)= 0.006116421364096079 \t (accuracy)= 98.85308688231132 % \t time= 31657.78785634041\n",
      "Test==: loss =  0.07476052589434565 \t accuracy= 88.89097040089922 %\n",
      "\n",
      "Train::: epoch= 132 \t lr= 0.01448321898683439 \t (loss)= 0.005741233247978783 \t (accuracy)= 98.91553224262104 % \t time= 31984.875814914703\n",
      "Test==: loss =  0.07417742017281385 \t accuracy= 89.00337204945673 %\n",
      "\n",
      "Train::: epoch= 133 \t lr= 0.01448321898683439 \t (loss)= 0.005702646344417208 \t (accuracy)= 98.79272303401191 % \t time= 32310.707748651505\n",
      "Test==: loss =  0.07458374455031927 \t accuracy= 89.22817534657175 %\n",
      "\n",
      "Train::: epoch= 134 \t lr= 0.01448321898683439 \t (loss)= 0.005527236883169237 \t (accuracy)= 98.88847258648681 % \t time= 32636.568684101105\n",
      "Test==: loss =  0.07536749061987057 \t accuracy= 89.00337204945673 %\n",
      "\n",
      "Train::: epoch= 135 \t lr= 0.01448321898683439 \t (loss)= 0.0054761447045377455 \t (accuracy)= 98.87598351442489 % \t time= 32959.3777859211\n",
      "Test==: loss =  0.07483348131926304 \t accuracy= 89.22817534657175 %\n",
      "\n",
      "Train::: epoch= 136 \t lr= 0.01448321898683439 \t (loss)= 0.005642964515218808 \t (accuracy)= 98.80729361808417 % \t time= 33282.47069144249\n",
      "Test==: loss =  0.07402002644635759 \t accuracy= 89.24690895466468 %\n",
      "\n",
      "Train::: epoch= 137 \t lr= 0.01448321898683439 \t (loss)= 0.0053909020603157805 \t (accuracy)= 98.92593980267266 % \t time= 33606.006736278534\n",
      "Test==: loss =  0.07505094336090536 \t accuracy= 89.02210565754964 %\n",
      "\n",
      "Train::: epoch= 138 \t lr= 0.01448321898683439 \t (loss)= 0.005434491319275116 \t (accuracy)= 98.89679863452811 % \t time= 33931.35873293877\n",
      "Test==: loss =  0.07379364892329485 \t accuracy= 89.30310977894342 %\n",
      "\n",
      "Train::: epoch= 139 \t lr= 0.01448321898683439 \t (loss)= 0.005377942489097567 \t (accuracy)= 98.94051038674493 % \t time= 34257.2474398613\n",
      "Test==: loss =  0.07354909006469885 \t accuracy= 89.5841139003372 %\n",
      "\n",
      "Train::: epoch= 140 \t lr= 0.01316656271530399 \t (loss)= 0.005123031775247956 \t (accuracy)= 99.00920028308563 % \t time= 34580.87045121193\n",
      "Test==: loss =  0.07199252805130416 \t accuracy= 89.94005245410266 %\n",
      "\n",
      "Train::: epoch= 141 \t lr= 0.01316656271530399 \t (loss)= 0.005078227377465489 \t (accuracy)= 98.98005911494108 % \t time= 34906.30784010887\n",
      "Test==: loss =  0.07204064206216369 \t accuracy= 89.6777819408018 %\n",
      "\n",
      "Train::: epoch= 142 \t lr= 0.01316656271530399 \t (loss)= 0.005601283940804708 \t (accuracy)= 98.91553224262104 % \t time= 35231.80065727234\n",
      "Test==: loss =  0.0733190751631662 \t accuracy= 89.5841139003372 %\n",
      "\n",
      "Train::: epoch= 143 \t lr= 0.01316656271530399 \t (loss)= 0.0052752096997405245 \t (accuracy)= 98.9967112110237 % \t time= 35557.64884972572\n",
      "Test==: loss =  0.07154567030984922 \t accuracy= 89.71524915698762 %\n",
      "\n",
      "Train::: epoch= 144 \t lr= 0.01316656271530399 \t (loss)= 0.00499818979999738 \t (accuracy)= 99.00503725906499 % \t time= 35881.946345090866\n",
      "Test==: loss =  0.07171163361864329 \t accuracy= 90.05245410266018 %\n",
      "\n",
      "Train::: epoch= 145 \t lr= 0.01316656271530399 \t (loss)= 0.005166966072641242 \t (accuracy)= 98.92385829066234 % \t time= 36208.14286136627\n",
      "Test==: loss =  0.07131211335137959 \t accuracy= 89.9775196702885 %\n",
      "\n",
      "Train::: epoch= 146 \t lr= 0.01316656271530399 \t (loss)= 0.004974566728297559 \t (accuracy)= 98.9967112110237 % \t time= 36533.20146203041\n",
      "Test==: loss =  0.07177773633231183 \t accuracy= 90.01498688647433 %\n",
      "\n",
      "Train::: epoch= 147 \t lr= 0.01316656271530399 \t (loss)= 0.004984061506057039 \t (accuracy)= 99.03209691519919 % \t time= 36859.46952366829\n",
      "Test==: loss =  0.07093539139326735 \t accuracy= 89.95878606219559 %\n",
      "\n",
      "Train::: epoch= 148 \t lr= 0.01316656271530399 \t (loss)= 0.004914240897449622 \t (accuracy)= 98.95508097081719 % \t time= 37184.960970401764\n",
      "Test==: loss =  0.0707281916856584 \t accuracy= 89.88385162982391 %\n",
      "\n",
      "Train::: epoch= 149 \t lr= 0.01316656271530399 \t (loss)= 0.005028538165299505 \t (accuracy)= 98.95299945880687 % \t time= 37512.14278960228\n",
      "Test==: loss =  0.07173553623433883 \t accuracy= 89.88385162982391 %\n",
      "\n",
      "Train::: epoch= 150 \t lr= 0.011969602468458171 \t (loss)= 0.004933943019802444 \t (accuracy)= 99.04458598726114 % \t time= 37836.54578924179\n",
      "Test==: loss =  0.07068276955296164 \t accuracy= 90.08992131884601 %\n",
      "\n",
      "Train::: epoch= 151 \t lr= 0.011969602468458171 \t (loss)= 0.004950494128607733 \t (accuracy)= 99.0237708671579 % \t time= 38162.352575302124\n",
      "Test==: loss =  0.07117060084383635 \t accuracy= 90.03372049456725 %\n",
      "\n",
      "Train::: epoch= 152 \t lr= 0.011969602468458171 \t (loss)= 0.00488456941123679 \t (accuracy)= 99.03417842720953 % \t time= 38488.63107538223\n",
      "Test==: loss =  0.0714588643382254 \t accuracy= 90.08992131884601 %\n",
      "\n",
      "Train::: epoch= 153 \t lr= 0.011969602468458171 \t (loss)= 0.004855336330980945 \t (accuracy)= 99.03001540318887 % \t time= 38815.263154268265\n",
      "Test==: loss =  0.07105679001458463 \t accuracy= 90.10865492693893 %\n",
      "\n",
      "Train::: epoch= 154 \t lr= 0.011969602468458171 \t (loss)= 0.004606562106662793 \t (accuracy)= 98.9967112110237 % \t time= 39141.993970394135\n",
      "Test==: loss =  0.07077174076412501 \t accuracy= 90.0711877107531 %\n",
      "\n",
      "Train::: epoch= 155 \t lr= 0.011969602468458171 \t (loss)= 0.004746341659552979 \t (accuracy)= 99.07580866741601 % \t time= 39468.01720952988\n",
      "Test==: loss =  0.0710193356250332 \t accuracy= 90.01498688647433 %\n",
      "\n",
      "Train::: epoch= 156 \t lr= 0.011969602468458171 \t (loss)= 0.004531889721106881 \t (accuracy)= 99.12992797968444 % \t time= 39794.68004441261\n",
      "Test==: loss =  0.07085532692571997 \t accuracy= 90.10865492693893 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-5a7a9ff838bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mminibatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_length\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_of_words\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# calculating the loss of batch of character sthat constrcut a words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mloss_char\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m#===============================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 862\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1405\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "# shuff_index=torch.LongTensor(485224).random_(0,485224)\n",
    "# train_data_2=train_data_1[:,shuff_index]\n",
    "# train_labels_2=train_labels_1[:,shuff_index]\n",
    "\n",
    "accu_plt=[]\n",
    "loss_plt=[]\n",
    "test_loss_plt=[]\n",
    "accuracy_test_plt=[]\n",
    "confusion_mtx_parameters=[]\n",
    "\n",
    "for epoch in range(1,1000):\n",
    "    # to activate dropout during training \n",
    "    lstm_net.train()\n",
    "    # divide the learning rate by 3 except after the first epoch\n",
    "    if epoch % 10==0:\n",
    "        my_lr = my_lr / 1.1\n",
    "\n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
    "    lstm_optimizer=torch.optim.SGD( lstm_net.parameters() , lr=my_lr )\n",
    "    # set the initial h and c to be the zero vector\n",
    "\n",
    "        \n",
    "    # set the running quatities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "    num_matches=0\n",
    "    \n",
    "    # loop across batch of words\n",
    "    for i in range(0,num_train_words):\n",
    "      # initilize the hidden state every word as the words ara independent \n",
    "    \n",
    "        h = torch.zeros(2,batch_of_words,  hidden_size).cuda()\n",
    "        c = torch.zeros(2,batch_of_words,  hidden_size).cuda()\n",
    "        h=h.to(device)\n",
    "        c=c.to(device)\n",
    "    \n",
    "        word_length= len(train_data_1 [i])\n",
    "    # Set the gradients to zeros\n",
    "        lstm_optimizer.zero_grad()\n",
    "\n",
    "        minibatch_words  =  train_data_1[i ]\n",
    "        minibatch_labels =  train_labels_1[ i]\n",
    "        # sending to GPU\n",
    "        minibatch_words=minibatch_words.to(device)\n",
    "        minibatch_labels=minibatch_labels.to(device)\n",
    "\n",
    "\n",
    "        # Detach to prevent from backpropagating all the way to the beginning\n",
    "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
    "        h=h.detach()\n",
    "        c=c.detach()\n",
    "        h=h.requires_grad_()\n",
    "        c=c.requires_grad_()\n",
    "\n",
    "        # forward pass\n",
    "        scores_char, h, c = lstm_net(minibatch_words.view(word_length,1), h, c)\n",
    "\n",
    "        # reshape before calculating the loss for easier slicing of mini batch of words\n",
    "        scores_char = scores_char.view(  word_length*batch_of_words , vocab_size)\n",
    "        minibatch_labels = minibatch_labels.contiguous()\n",
    "        minibatch_labels = minibatch_labels.view(word_length*batch_of_words )\n",
    "        # calculating the loss of batch of character sthat constrcut a words\n",
    "        loss_char= criterion(scores_char, minibatch_labels)\n",
    "\n",
    "        #===============================================\n",
    "\n",
    "        # summation of both lossses to \n",
    "        combined_loss=loss_char\n",
    "\n",
    "\n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        combined_loss.backward()\n",
    "\n",
    "        # update the wights \n",
    "        lstm_optimizer.step()\n",
    "\n",
    "\n",
    "        # update the running loss  \n",
    "        running_loss += combined_loss.detach().item()\n",
    "        num_batches += 1\n",
    "        num_matches += get_error(scores_char, minibatch_labels)\n",
    "        \n",
    "        # end of iteration \n",
    "        \n",
    "    # compute for full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    loss_plt.append(total_loss)\n",
    "    accuracy= (num_matches/num_train_words)*100\n",
    "    accu_plt.append(accuracy)\n",
    "    # Compute the time \n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    print('')\n",
    "    print('Train:::', 'epoch=',epoch,'\\t lr=', my_lr, '\\t (loss)=',(total_loss),'\\t (accuracy)=' , (accuracy),'%','\\t time=', elapsed)\n",
    "    \n",
    "    # evaluate on test set to monitor the loss \n",
    "    acc_tst_plt, loss_tst_plt, confusion_param=eval_on_test_set()\n",
    "    \n",
    "    # saving test parameters  \n",
    "    test_loss_plt.append(loss_tst_plt)\n",
    "    accuracy_test_plt.append(acc_tst_plt)\n",
    "    confusion_mtx_parameters.append(confusion_param)\n",
    "    #### Saving model parameters every epoch \n",
    "    save_model_parameters() \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []], [[1.0], [1.0], [1.0], [0.0], []]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_mtx_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('lstm_single_char_50_mutation_bid_drop.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andriashfahani/miniconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Recurrent_Layer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(lstm_net, 'this_morning_w_do.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint['test_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.save(lstm_net,'single_mistake_model_lstm_drop_out_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('lstm_single_char_50_mutation_bid_drop_02.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.008481162786483764, 0.009953457117080688, 0.008462512493133545, 0.008568865060806275, 0.008872705698013305, 0.009778851270675659, 0.009399718046188355, 0.009138405323028564, 0.008280891180038451, 0.00938764214515686, 0.009248465299606323, 0.010755127668380738, 0.009009695053100586, 0.008971762657165528, 0.01076977252960205, 0.01007135510444641, 0.010158282518386842, 0.008264505863189697, 0.008282959461212158, 0.01121429204940796, 0.008798831701278686, 0.009055852890014648, 0.010330718755722047, 0.007766813039779663, 0.009448951482772827, 0.009692203998565675, 0.00912320613861084, 0.009875631332397461, 0.008551460504531861, 0.01096159815788269, 0.009085208177566528, 0.009600162506103516, 0.00832710862159729, 0.010008686780929565, 0.009891873598098755, 0.010521680116653442, 0.010294747352600098, 0.009758317470550537, 0.010009801387786866, 0.008804881572723388, 0.008309823274612427, 0.009526288509368897, 0.008488410711288452, 0.008953988552093506, 0.009343469142913818, 0.00920448899269104, 0.010769516229629517, 0.007997560501098632, 0.010012930631637574, 0.008238482475280761, 0.007914996147155762, 0.008441448211669922, 0.008660966157913208, 0.008735442161560058, 0.01002773642539978, 0.007890093326568603, 0.0065487980842590336, 0.009454089403152465, 0.011431741714477538, 0.007453322410583496, 0.009435546398162842, 0.007637321949005127, 0.008381032943725586, 0.009216135740280152]\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint[])\n",
    "#  'incorrect_to_correct':confusion_mtx_parameters[0],  'correct_to_correct':confusion_mtx_parameters[2],\n",
    "# #                 'correct_to_incorrect':confusion_mtx_parameters[1],'regenerate':confusion_mtx_parameters[3]\n",
    "  \n",
    "           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
