{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_Genration and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing built-in Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv as csv\n",
    "from random import randint\n",
    "import re\n",
    "import torch \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2data(fileStr):\n",
    "    dataStr = csv.reader(open(fileStr), delimiter='\\n', quotechar='|')\n",
    "    data = []\n",
    "    for row in dataStr:\n",
    "        eachRow = ','.join(row);\n",
    "        rowArr = np.asarray(eachRow.split(','));\n",
    "        data.append(rowArr[-1])\n",
    "    data = data[1:];\n",
    "    \n",
    "    return data\n",
    "\n",
    "def csv2images(fileStr):\n",
    "    dataStr = csv.reader(open(fileStr), delimiter='\\n', quotechar='|')\n",
    "    data = []\n",
    "    next(dataStr)\n",
    "    for row in dataStr:\n",
    "        eachRow = ','.join(row);\n",
    "        rowArr = list(map(int, eachRow.split(',')));\n",
    "        data.append(rowArr)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def cleanDict(dictionary):\n",
    "    newDict = [];\n",
    "    for word in dictionary:\n",
    "        if((len(list(word))>=4) and (len(word.split(' '))==1) and re.match(\"^[a-zA-Z]*$\", word) and re.match(\"^[^j|J|z|Z]*$\", word)):\n",
    "            newDict.append(word);\n",
    "    \n",
    "    return newDict        \n",
    "\n",
    "def getWordList(*arg):\n",
    "    \n",
    "    dictionary = arg[0]\n",
    "    if len(arg) == 1:\n",
    "        numOfFeatures = 100;\n",
    "    elif len(arg) == 2:\n",
    "        numOfFeatures = arg[1];\n",
    "    elif len(arg) == 3:\n",
    "        numOfFeatures = arg[1];\n",
    "        numOfMutations = arg[2];\n",
    "        \n",
    "    wordList =[];\n",
    "    \n",
    "    for word in dictionary:\n",
    "        row = [];\n",
    "        for fea in range(numOfFeatures):\n",
    "            if len(arg) < 3:\n",
    "                row.append(genMisspellings(word))\n",
    "            elif len(arg) == 3:\n",
    "                row.append(genMisspellings(word,numOfMutations))\n",
    "                \n",
    "        row.append(word)\n",
    "        wordList.append(row)\n",
    "    \n",
    "    return wordList\n",
    "\n",
    "def getSeparatedWordList(*arg):\n",
    "    \n",
    "    dictionary = arg[0];\n",
    "    testPercentage = 0.1;\n",
    "    truePercenage = 0.2;\n",
    "        \n",
    "    if len(arg) == 1:\n",
    "        numOfFeatures = 100;\n",
    "    elif len(arg) == 2:\n",
    "        numOfFeatures = arg[1];\n",
    "    elif len(arg) == 3:\n",
    "        numOfFeatures = arg[1];\n",
    "        numOfMutations = arg[2];\n",
    "    elif len(arg) == 4:\n",
    "        numOfFeatures = arg[1];\n",
    "        numOfMutations = arg[2];\n",
    "        testPercentage = arg[3];\n",
    "    elif len(arg) == 5:\n",
    "        numOfFeatures = arg[1];\n",
    "        numOfMutations = arg[2];\n",
    "        testPercentage = arg[3];\n",
    "        truePercenage = arg[4];\n",
    "        \n",
    "    trainWordList =[];\n",
    "    testWordList = [];\n",
    "        \n",
    "    noOfMisspellings = math.floor((1-truePercenage)*numOfFeatures);\n",
    "    noOfTestSamples = math.floor(testPercentage*numOfFeatures);\n",
    "    \n",
    "    for word in dictionary:\n",
    "        row = [];\n",
    "        for fea in range(noOfMisspellings):\n",
    "            if len(arg) < 3:\n",
    "                row.append(genMisspellings(word))\n",
    "            elif len(arg) >= 3:\n",
    "                row.append(genMisspellings(word,numOfMutations))\n",
    "        \n",
    "        trainRow, testRow = train_test_split(row,test_size=testPercentage);\n",
    "        \n",
    "        trainRow.extend([word]*math.ceil((numOfFeatures-noOfMisspellings)*(1-testPercentage)));\n",
    "        testRow.extend([word]*math.ceil((numOfFeatures-noOfMisspellings)*(testPercentage)));       \n",
    "        \n",
    "        trainWordList.append(trainRow)\n",
    "        testWordList.append(testRow)\n",
    "        \n",
    "    return trainWordList,testWordList\n",
    "\n",
    "\n",
    "def genMisspellings(*arg):\n",
    "    \n",
    "    word = arg[0]\n",
    "    minMut = 0;\n",
    "    if len(arg) == 1:        \n",
    "        maxMut = randint(1,int(len(word)/2));\n",
    "    else:\n",
    "        maxMut = arg[1]\n",
    "    \n",
    "    alphabetArr = ['a','b','c','d','e','f','g','h','i','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y'];\n",
    "    charArr = list(word);\n",
    "    \n",
    "    for mut in range(minMut,maxMut):        \n",
    "        mutIdx = randint(0,len(word)-1);\n",
    "        charArr[mutIdx] = alphabetArr[randint(0,23)];    \n",
    "\n",
    "    return \"\".join(charArr)\n",
    "\n",
    "def createBuckets(trainImages):\n",
    "    buckets = [[] for _ in range(25)];\n",
    "\n",
    "    for idx, sample in enumerate(trainImages):\n",
    "        buckets[int(sample[0])].append(idx);\n",
    "        \n",
    "    return buckets\n",
    "\n",
    "def generateData(wordList,buckets):\n",
    "    \n",
    "    data = [];\n",
    "    for sample in wordList:\n",
    "        label = sample[-1];\n",
    "        for word in sample:\n",
    "            refArr = [ord(char) - 96 for char in word.lower()];\n",
    "            idxArr = [];\n",
    "            for ele in refArr:\n",
    "                idxArr.append(buckets[ele-1][randint(0,len(buckets[ele-1])-1)]);\n",
    "            data.append(list([word,idxArr,label]));    \n",
    "\n",
    "    return data\n",
    "\n",
    "def generateTrueData(wordList,buckets):\n",
    "    \n",
    "    data = [];\n",
    "    for word in wordList:       \n",
    "        refArr = [ord(char) - 96 for char in word.lower()];\n",
    "        idxArr = [];\n",
    "        for ele in refArr:\n",
    "            idxArr.append(buckets[ele-1][randint(0,len(buckets[ele-1])-1)]);\n",
    "        data.append(list([word,idxArr]));    \n",
    "\n",
    "    return data\n",
    "\n",
    "def generateLSTMData(wordList,buckets):\n",
    "    \n",
    "    data = [];\n",
    "    for sample in wordList:\n",
    "        label = sample[-1];\n",
    "        labelArr = [ord(char) - 96 for char in label.lower()]\n",
    "        for word in sample:\n",
    "            refArr = [ord(char) - 96 for char in word.lower()]\n",
    "            idxArr = []\n",
    "            for ele in refArr:\n",
    "                idxArr.append(buckets[ele-1][randint(0,len(buckets[ele-1])-1)]);\n",
    "            data.append((word,refArr,label,labelArr));    \n",
    "\n",
    "    return data\n",
    "\n",
    "def Gen(listOfwords, trainImages):\n",
    "    \n",
    "    # In[13]:\n",
    "    \n",
    "    # create buckets containing the indices of the corresponding alphabets\n",
    "    buckets = createBuckets(trainImages);\n",
    "    \n",
    "    # remove words with 'j,J,z,Z' or any special characters\n",
    "    dictionary = cleanDict(listOfwords);\n",
    "    \n",
    "    # generate misspelled words\n",
    "    # getData(dictionary,numOfFeatures,numOfMutations)\n",
    "    # OR\n",
    "    # getData(dictionary)\n",
    "    wordList = getWordList(dictionary,10,1)\n",
    "    \n",
    "    # generate data with misspelled word, index of location of each of the alphabet of the incorrect word and the correct word\n",
    "    data = generateData(wordList,buckets);\n",
    "    \n",
    "    # generate data with word, index of location of each of the alphabet of the word\n",
    "    #data = generateTrueData(dictionary,buckets);\n",
    "\n",
    "    return data\n",
    "\n",
    "def GenSplitData(listOfwords, trainImages, testImages, numOfFeatures, numOfMutations, testPercentage, truePercentage):\n",
    "    \n",
    "    # In[13]:\n",
    "    \n",
    "    # create buckets containing the indices of the corresponding alphabets\n",
    "    trainBuckets = createBuckets(trainImages);\n",
    "    testBuckets = createBuckets(testImages);\n",
    "    \n",
    "    # remove words with 'j,J,z,Z' or any special characters\n",
    "    dictionary = cleanDict(listOfwords);\n",
    "    \n",
    "    # generate misspelled words\n",
    "    # getData(dictionary,numOfFeatures,numOfMutations)\n",
    "    # OR\n",
    "    # getData(dictionary)\n",
    "    # OR\n",
    "    # getSeparatedWordList(dictionary,numOfFeatures,numOfMutations,testPercentage,truePercentage)\n",
    "    trainWordList, testWordList = getSeparatedWordList(dictionary, numOfFeatures, numOfMutations, testPercentage, truePercentage)\n",
    "    \n",
    "    # generate data with misspelled word, index of location of each of the alphabet of the incorrect word and the correct word\n",
    "    trainData = generateData(trainWordList,trainBuckets);\n",
    "    testData = generateData(testWordList,testBuckets);\n",
    "    \n",
    "    # generate data with word, index of location of each of the alphabet of the word\n",
    "    #data = generateTrueData(dictionary,buckets);\n",
    "\n",
    "    return trainData, testData\n",
    "\n",
    "def split_indexes(full_data):\n",
    "    \n",
    "    item_1=[item[1] for item in full_data]\n",
    "    idxs=([torch.LongTensor(xi) for xi in item_1])\n",
    "    item_3=[item[3] for item in full_data]\n",
    "    labels=([torch.LongTensor(xi) for xi in item_3])\n",
    "    \n",
    "    return idxs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfwords = csv2data('Misspelled_correct_words_data.csv')\n",
    "\n",
    "trainImages = csv2images('sign_mnist_train.csv')\n",
    "\n",
    "testImages = csv2images('sign_mnist_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genrate Data for LSTM_Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = cleanDict(listOfwords);\n",
    "trainWordList, testWordList = getSeparatedWordList(dictionary, 200, 1, 0.1, 1.0)\n",
    "\n",
    "trainBuckets = createBuckets(trainImages)\n",
    "testBuckets = createBuckets(testImages)\n",
    "\n",
    "trainData = generateLSTMData(trainWordList,trainBuckets)\n",
    "testData = generateLSTMData(testWordList,testBuckets)\n",
    "\n",
    "train_index, train_labels =split_indexes(trainData)\n",
    "test_index, test_labels=split_indexes(testData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_index,'train_data_20_mut_2_c.pt')\n",
    "torch.save(train_labels,'train_labels_20_mut_2_c.pt')\n",
    "torch.save(test_index,'test_data_20_mut_2_c.pt')\n",
    "torch.save(test_labels,'test_labels_20_mut_2_c.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create buckets containing the indices of the corresponding alphabets\n",
    "buckets = createBuckets(trainImages);\n",
    "\n",
    "# remove words with 'j,J,z,Z' or any special characters\n",
    "dictionary = cleanDict(listOfwords);\n",
    "\n",
    "# generate misspelled words\n",
    "# getData(dictionary,numOfFeatures,numOfMutations)\n",
    "# OR\n",
    "# getData(dictionary)\n",
    "wordList = getWordList(dictionary,100,1)\n",
    "\n",
    "\n",
    "# generate data with misspelled word, index of location of each of the alphabet of the incorrect word and the correct word\n",
    "#data = generateData(wordList,buckets);\n",
    "\n",
    "# generate data with word, index of location of each of the alphabet of the word\n",
    "#data = generateTrueData(dictionary,buckets);\n",
    "\n",
    "# getSeparatedWordList(dictionary,numOfFeatures,numOfMutations,testPercentage,truePercentage)\n",
    "#trainwordList, testwordList = getSeparatedWordList(dictionary, 20, 2, 0.1, 0.3)\n",
    "\n",
    "# generate data with misspelled word, index of location of each of the alphabet of the incorrect word and the correct word\n",
    "data = generateLSTMData(wordList,buckets);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generate Data for END-End Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_to_train_imgs = Gen(listOfwords, trainImages)\n",
    "word_to_test_imgs = Gen(listOfwords, testImages)\n",
    "\n",
    "#### now we split both list in equal proportions and same indices...so each proportion contains same words but \n",
    "####   different images (train or test imgs) \n",
    "trainWordstrainImages, testWordstrainImages, trainWordstestImages, testWordstestImages = train_test_split(word_to_train_imgs, word_to_test_imgs, test_size = 0.30, random_state = 101)\n",
    "\n",
    "torch.save(trainWordstrainImages, 'trainWordstrainImages.pt')  #### only this is used to train CLSTM, others to be used for testing all models\n",
    "torch.save(testWordstrainImages, 'testWordstrainImages.pt')\n",
    "torch.save(trainWordstestImages, 'trainWordstestImages.pt')\n",
    "torch.save(testWordstestImages, 'testWordstestImages.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_to_train_imgs, word_to_test_imgs = GenSplitData(listOfwords, trainImages, testImages, numOfFeatures,numOfMutations,testPercentage,truePercentage)\n",
    "word_to_train_imgs, word_to_test_imgs = GenSplitData(listOfwords, trainImages, testImages, 102, 1, 0.99, 1.0)\n",
    "\n",
    "#### now we split both list in equal proportions and same indices...so each proportion contains same words but \n",
    "####   different images (train or test imgs) \n",
    "#trainWordstrainImages, testWordstrainImages, trainWordstestImages, testWordstestImages = train_test_split(word_to_train_imgs, word_to_test_imgs, test_size = 0.30, random_state = 101)\n",
    "\n",
    "torch.save(word_to_train_imgs, 'trainWordstrainImages.pt')  #### only this is used to train CLSTM, others to be used for testing all models\n",
    "#torch.save(testWordstrainImages, 'testWordstrainImages.pt')\n",
    "#torch.save(trainWordstestImages, 'trainWordstestImages.pt')\n",
    "torch.save(word_to_test_imgs, 'testWordstestImages.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Torch Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the data \n",
    "misspelled_index_pad=pad_sequence(misspelled_index)\n",
    "mis_word_pad=torch.transpose(mis_word_pad,1,0)\n",
    "# padding the labels\n",
    "correct_word_pad=pad_sequence(correct_index)\n",
    "correct_word_pad=torch.transpose(correct_word_pad,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch Splitting to Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing into test sets and training sets 10 % percent for test or evaluating set\n",
    "# taking ten percent of data as atest set\n",
    "train_size = int(0.9 * len(mis_word_pad))\n",
    "test_size = len(mis_word_pad) - train_size\n",
    "train, test = torch.utils.data.random_split(mis_word_pad, [train_size, test_size])\n",
    "train_lbl, test_lbl   = random_split(correct_word_pad,[train_size, test_size])\n",
    "\n",
    "# splitting with random indices \n",
    "train_data=train.dataset[train.indices]\n",
    "test_data=test.dataset[test.indices]\n",
    "train_labels=train_lbl.dataset[train.indices]\n",
    "test_labels=test_lbl.dataset[test.indices]\n",
    "# saving data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Saving the Data diffrent Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving as a data frame \n",
    "df = pd.DataFrame(data, columns=['Mis_word', 'Mis_idx', 'correc_word','correc_idx']) \n",
    "df.to_csv('LSTM_data.csv', index=False, encoding='utf-8')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch  Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(misspelled_words, 'misspelled_words.pt')\n",
    "torch.save(misspelled_index, 'misspelled_index.pt')\n",
    "torch.save(correct_words, 'correct_words.pt')\n",
    "torch.save(correct_index, 'correct_index.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
