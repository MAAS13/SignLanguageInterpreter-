{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import unicodedata\n",
    "import string \n",
    "import re\n",
    "import time \n",
    "from torch.autograd import Variable\n",
    "import math \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData=torch.load('train_data_50_mut_1_c.pt')\n",
    "trainLabel=torch.load('train_labels_50_mut_1_c.pt')\n",
    "testData=torch.load('test_data_50_mut_1_c.pt')\n",
    "testLabel=torch.load('test_labels_50_mut_1_c.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_words= 9608    #48042 # 90% for trianing \n",
    "test_words= 1068#5338 # 10% for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Architecture \n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding=nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm=nn.LSTM(hidden_size, hidden_size )\n",
    "    \n",
    "    def forward(self, x, h_init, c_init):\n",
    "        \n",
    "        g_seq=self.embedding(x)\n",
    "        output, (h_final,c_final)=self.lstm(g_seq, (h_init, c_init))\n",
    "\n",
    "        return output, h_final, c_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Architecture \n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm=nn.LSTM(hidden_size, hidden_size)\n",
    "        self.out=nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h_init, c_init):\n",
    "        output=self.embedding(x)\n",
    "        output, (h_final, c_final)=self.lstm(output, (h_init, c_init))\n",
    "        output=self.out(output) \n",
    "        return output, h_final, c_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderLSTM(\n",
      "  (embedding): Embedding(26, 256)\n",
      "  (lstm): LSTM(256, 256)\n",
      ")\n",
      "EncoderLSTM(\n",
      "  (embedding): Embedding(26, 256)\n",
      "  (lstm): LSTM(256, 256)\n",
      ")\n",
      "DecoderLSTM(\n",
      "  (embedding): Embedding(26, 256)\n",
      "  (lstm): LSTM(256, 256)\n",
      "  (out): Linear(in_features=256, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# vocablulary size 26 charcaters +sow and eow tokens\n",
    "hidden_size=256\n",
    "vocab_size=26\n",
    "encoder=EncoderLSTM(vocab_size,hidden_size).cuda()\n",
    "decoder=DecoderLSTM(hidden_size,vocab_size).cuda()\n",
    "my_lr=0.04\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "SOW_token=0\n",
    "EOW_token=27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error( scores , labels ):\n",
    "\n",
    "    bs=scores.size(0)\n",
    "    predicted_labels = scores.argmax(dim=1)\n",
    "    indicator = (predicted_labels == labels)\n",
    "    num_matches=indicator.sum()\n",
    "    num_words=num_matches/len(predicted_labels)\n",
    "    return num_words.item()\n",
    "\n",
    "\n",
    "def save_model_parameters():\n",
    "# save models \n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'encoder_model': encoder.state_dict(),\n",
    "                'decoder_model':decoder.state_dict(),\n",
    "                'encoder_optimizer': encoder_optimizer.state_dict(),\n",
    "                'decoder_optimizer': decoder_optimizer.state_dict(),\n",
    "                'train_loss': loss_plt, 'train_accuracy': accu_plt, \n",
    "                'test_loss': test_loss_plt,\n",
    "                'test_accuracy': accuracy_test_plt,\n",
    "                'confusion matrix_parameters':cnt_list },  'seq_seq_model_sinlgle_char_50_mutation.pt')\n",
    "#                 'incorrect_to_correct':cnt_list[0],  'correct_to_correct':cnt_list[2],\n",
    "#                 'correct_to_incorrect':cnt_list[1],'regenerate':cnt_list[3] \n",
    "                  \n",
    "def confusion_parameters(scores,target_tensor,inpute_tensor):\n",
    "    \n",
    "    conf_counter= torch.zeros(4)\n",
    "\n",
    "  \n",
    "    if torch.all(torch.eq(target_tensor, scores.argmax(dim=1)))==1 and torch.all(torch.eq(inpute_tensor,target_tensor))==0: ### making incorrect->correct\n",
    "        conf_counter[0] +=1\n",
    "\n",
    "    if torch.all(torch.eq(inpute_tensor, scores.argmax(dim=1)))==0 and torch.all(torch.eq(inpute_tensor,target_tensor))==1: ### making correct->incorrect\n",
    "\n",
    "        conf_counter[1] +=1    \n",
    "\n",
    "\n",
    "    if torch.all(torch.eq(inpute_tensor, scores.argmax(dim=1)))==1 and torch.all(torch.eq(inpute_tensor,target_tensor))==1: ### making correct->correct\n",
    "\n",
    "        conf_counter[2] +=1                                                               \n",
    "\n",
    "    if torch.all(torch.eq(inpute_tensor, scores.argmax(dim=1)))==1 and torch.all(torch.eq(inpute_tensor,target_tensor))==0: ### making inccorrect->correct or regenerate the input \n",
    "\n",
    "        conf_counter[3] +=1  \n",
    "    return conf_counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    loss_word=0\n",
    "    run_error=0.0\n",
    "    loss_plt=[]\n",
    "    acc_plt=[]\n",
    "    # counts for computing correct and incorrect proportion\n",
    "    count=torch.zeros(4)\n",
    "    cnf_mtx_count=torch.zeros(4)\n",
    "    \n",
    "    for count in range(test_words):\n",
    "        loss_char=0\n",
    "        input_tensor = testData[count].cuda()\n",
    "        target_tensor= testLabel[count].cuda()\n",
    "\n",
    "        # input and target words length\n",
    "        input_length = input_tensor.size(0)\n",
    "        # checkking the correct accuracy\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "        #initial hidden states\n",
    "        encoder_h = Variable(torch.zeros(1, 1,hidden_size))\n",
    "        encoder_c = Variable(torch.zeros(1, 1,hidden_size))\n",
    "\n",
    "        #send to GPU\n",
    "        encoder_h=encoder_h.to(device)\n",
    "        encoder_c=encoder_c.to(device)\n",
    "\n",
    "        #encoding\n",
    "        encoder_out,encoder_h,encoder_c = encoder(input_tensor.view(input_length,1), encoder_h, encoder_c)\n",
    "\n",
    "        # transfering feature vector from encoder to decoder\n",
    "        decoder_h = encoder_h \n",
    "        decoder_c = encoder_c\n",
    "\n",
    "        #initial input decoder\n",
    "        decoder_input=torch.LongTensor([SOW_token]).cuda()\n",
    "\n",
    "        # decoding\n",
    "        # outputs t ozero\n",
    "        outputs = torch.zeros(input_length, vocab_size).to(device)\n",
    "\n",
    "       \n",
    "        for dc in range(input_length):\n",
    "            decoder_out, deocder_h, decoder_c= decoder(decoder_input.view(1,1),decoder_h,decoder_c)\n",
    "\n",
    "            outputs[dc]=decoder_out\n",
    "            top1 = decoder_out[0][0].argmax()\n",
    "            decoder_input=top1\n",
    "            loss_char += criterion(decoder_out.view(1,vocab_size),target_tensor[dc].unsqueeze(0)) \n",
    "        \n",
    "        run_error+=get_error( outputs , target_tensor)\n",
    "        loss_word+= loss_char/input_length \n",
    "        \n",
    "         #=======================================================#\n",
    "        # compute confusion paramters \n",
    "        cnf_mtx_count+=confusion_parameters(outputs,target_tensor,input_tensor)\n",
    "        #============================================# \n",
    "       \n",
    "         ###====Different Metrics for evaluation=============###\n",
    "    \n",
    "    # counter for computing confusion matrix\n",
    "\n",
    "    \n",
    "    # accuracy and loss \n",
    "    total_loss = loss_word/test_words\n",
    "    loss_plt.append(total_loss)\n",
    "    accuracy= (run_error/test_words)*100\n",
    "    acc_plt.append(accuracy)\n",
    "    print('test::: (loss) = ', (total_loss.item()) ,'\\t (accuracy)=' , (accuracy),'%')\n",
    "    return loss_plt, acc_plt, cnf_mtx_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train::: epoch= 1 \t lr= 0.04 \t (loss)= 0.17199699144068026 \t (accuracy)= 92.64154870940882 % \t time= 141.53442645072937\n",
      "test::: (loss) =  6.010216236114502 \t (accuracy)= 0.7490636704119851 %\n",
      "Train::: epoch= 2 \t lr= 0.04 \t (loss)= 0.12507828081167174 \t (accuracy)= 93.61990008326396 % \t time= 289.28952980041504\n",
      "test::: (loss) =  5.488241672515869 \t (accuracy)= 1.8726591760299627 %\n",
      "Train::: epoch= 3 \t lr= 0.04 \t (loss)= 0.10933578504959705 \t (accuracy)= 94.4525395503747 % \t time= 434.35473346710205\n",
      "test::: (loss) =  5.096492767333984 \t (accuracy)= 2.0599250936329585 %\n",
      "Train::: epoch= 4 \t lr= 0.04 \t (loss)= 0.09487209790487784 \t (accuracy)= 94.85845129059118 % \t time= 579.6163506507874\n",
      "test::: (loss) =  5.028802871704102 \t (accuracy)= 2.3408239700374533 %\n",
      "Train::: epoch= 5 \t lr= 0.04 \t (loss)= 0.08048147634310013 \t (accuracy)= 95.16028309741881 % \t time= 725.3104503154755\n",
      "test::: (loss) =  5.318463325500488 \t (accuracy)= 2.4344569288389515 %\n",
      "Train::: epoch= 6 \t lr= 0.04 \t (loss)= 0.07865689085521452 \t (accuracy)= 95.22273105745212 % \t time= 869.9768016338348\n",
      "test::: (loss) =  5.252290725708008 \t (accuracy)= 2.9962546816479403 %\n",
      "Train::: epoch= 7 \t lr= 0.04 \t (loss)= 0.07823827220168011 \t (accuracy)= 95.39966694421315 % \t time= 1016.1482946872711\n",
      "test::: (loss) =  5.2278218269348145 \t (accuracy)= 3.3707865168539324 %\n",
      "Train::: epoch= 8 \t lr= 0.04 \t (loss)= 0.06657654085110583 \t (accuracy)= 95.8992506244796 % \t time= 1160.872410774231\n",
      "test::: (loss) =  4.963213920593262 \t (accuracy)= 4.49438202247191 %\n",
      "Train::: epoch= 9 \t lr= 0.04 \t (loss)= 0.06365646123136169 \t (accuracy)= 96.27393838467944 % \t time= 1306.819943189621\n",
      "test::: (loss) =  5.003042221069336 \t (accuracy)= 6.460674157303371 %\n",
      "Train::: epoch= 10 \t lr= 0.03636363636363636 \t (loss)= 0.06840427422858829 \t (accuracy)= 96.09700249791841 % \t time= 1452.6025640964508\n",
      "test::: (loss) =  4.717365264892578 \t (accuracy)= 7.303370786516854 %\n",
      "Train::: epoch= 11 \t lr= 0.03636363636363636 \t (loss)= 0.052263584474204594 \t (accuracy)= 96.91923397169026 % \t time= 1598.2821912765503\n",
      "test::: (loss) =  4.817180156707764 \t (accuracy)= 9.737827715355806 %\n",
      "Train::: epoch= 12 \t lr= 0.03636363636363636 \t (loss)= 0.04198832149576102 \t (accuracy)= 97.52289758534555 % \t time= 1743.3978550434113\n",
      "test::: (loss) =  4.485993385314941 \t (accuracy)= 12.921348314606742 %\n",
      "Train::: epoch= 13 \t lr= 0.03636363636363636 \t (loss)= 0.0447447013044453 \t (accuracy)= 97.57493755203997 % \t time= 1889.7955133914948\n",
      "test::: (loss) =  4.425441265106201 \t (accuracy)= 12.640449438202248 %\n",
      "Train::: epoch= 14 \t lr= 0.03636363636363636 \t (loss)= 0.04434178860941019 \t (accuracy)= 97.58534554537886 % \t time= 2035.3671624660492\n",
      "test::: (loss) =  4.088830947875977 \t (accuracy)= 13.857677902621724 %\n",
      "Train::: epoch= 15 \t lr= 0.03636363636363636 \t (loss)= 0.03796482808279054 \t (accuracy)= 97.90799333888425 % \t time= 2181.6364855766296\n",
      "test::: (loss) =  4.414882183074951 \t (accuracy)= 14.794007490636703 %\n",
      "Train::: epoch= 16 \t lr= 0.03636363636363636 \t (loss)= 0.03292291609108399 \t (accuracy)= 98.03288925895087 % \t time= 2326.887102365494\n",
      "test::: (loss) =  4.298746585845947 \t (accuracy)= 14.325842696629213 %\n",
      "Train::: epoch= 17 \t lr= 0.03636363636363636 \t (loss)= 0.0405061075615296 \t (accuracy)= 97.90799333888425 % \t time= 2472.1029620170593\n",
      "test::: (loss) =  3.9854161739349365 \t (accuracy)= 20.880149812734082 %\n",
      "Train::: epoch= 18 \t lr= 0.03636363636363636 \t (loss)= 0.03983028264614134 \t (accuracy)= 97.73105745212322 % \t time= 2617.3750190734863\n",
      "test::: (loss) =  3.9779646396636963 \t (accuracy)= 19.00749063670412 %\n",
      "Train::: epoch= 19 \t lr= 0.03636363636363636 \t (loss)= 0.035004007700130776 \t (accuracy)= 98.10574521232306 % \t time= 2764.1918036937714\n",
      "test::: (loss) =  3.984405755996704 \t (accuracy)= 20.880149812734082 %\n",
      "Train::: epoch= 20 \t lr= 0.033057851239669415 \t (loss)= 0.03601328405258543 \t (accuracy)= 98.19941715237303 % \t time= 2909.4650094509125\n",
      "test::: (loss) =  3.2394797801971436 \t (accuracy)= 30.243445692883896 %\n",
      "Train::: epoch= 21 \t lr= 0.033057851239669415 \t (loss)= 0.032655249912697755 \t (accuracy)= 98.43880099916737 % \t time= 3054.4458618164062\n",
      "test::: (loss) =  3.3213388919830322 \t (accuracy)= 30.430711610486895 %\n",
      "Train::: epoch= 22 \t lr= 0.033057851239669415 \t (loss)= 0.030157214885321335 \t (accuracy)= 98.44920899250624 % \t time= 3200.365174293518\n",
      "test::: (loss) =  3.1584250926971436 \t (accuracy)= 32.77153558052434 %\n",
      "Train::: epoch= 23 \t lr= 0.033057851239669415 \t (loss)= 0.026263324294715513 \t (accuracy)= 98.59492089925062 % \t time= 3345.81663441658\n",
      "test::: (loss) =  3.1551513671875 \t (accuracy)= 35.86142322097378 %\n",
      "Train::: epoch= 24 \t lr= 0.033057851239669415 \t (loss)= 0.03290585719142156 \t (accuracy)= 98.53247293921731 % \t time= 3492.136709213257\n",
      "test::: (loss) =  2.8736495971679688 \t (accuracy)= 40.074906367041194 %\n",
      "Train::: epoch= 25 \t lr= 0.033057851239669415 \t (loss)= 0.021194008575825127 \t (accuracy)= 98.71981681931724 % \t time= 3637.791166782379\n",
      "test::: (loss) =  2.5887668132781982 \t (accuracy)= 44.9438202247191 %\n",
      "Train::: epoch= 26 \t lr= 0.033057851239669415 \t (loss)= 0.019435205543722894 \t (accuracy)= 98.91756869275603 % \t time= 3783.657722234726\n",
      "test::: (loss) =  2.663468837738037 \t (accuracy)= 45.41198501872659 %\n",
      "Train::: epoch= 27 \t lr= 0.033057851239669415 \t (loss)= 0.017666403407108858 \t (accuracy)= 99.08409658617818 % \t time= 3929.4836444854736\n",
      "test::: (loss) =  2.5667834281921387 \t (accuracy)= 48.59550561797753 %\n",
      "Train::: epoch= 28 \t lr= 0.033057851239669415 \t (loss)= 0.020817062812861078 \t (accuracy)= 98.96960865945046 % \t time= 4075.401559114456\n",
      "test::: (loss) =  2.911775827407837 \t (accuracy)= 42.50936329588015 %\n",
      "Train::: epoch= 29 \t lr= 0.033057851239669415 \t (loss)= 0.024414427669184464 \t (accuracy)= 98.79267277268943 % \t time= 4220.89812541008\n",
      "test::: (loss) =  2.3377621173858643 \t (accuracy)= 51.31086142322098 %\n",
      "Train::: epoch= 30 \t lr= 0.030052592036063103 \t (loss)= 0.01866363068338148 \t (accuracy)= 99.21940049958367 % \t time= 4366.50620341301\n",
      "test::: (loss) =  2.3333261013031006 \t (accuracy)= 53.558052434456926 %\n",
      "Train::: epoch= 31 \t lr= 0.030052592036063103 \t (loss)= 0.018269177337842418 \t (accuracy)= 99.02164862614488 % \t time= 4511.454784154892\n",
      "test::: (loss) =  2.090489149093628 \t (accuracy)= 57.11610486891385 %\n",
      "Train::: epoch= 32 \t lr= 0.030052592036063103 \t (loss)= 0.01709956143760665 \t (accuracy)= 99.11532056619484 % \t time= 4656.62332701683\n",
      "test::: (loss) =  1.901951789855957 \t (accuracy)= 61.329588014981276 %\n",
      "Train::: epoch= 33 \t lr= 0.030052592036063103 \t (loss)= 0.014700138749806494 \t (accuracy)= 99.21940049958367 % \t time= 4802.161602020264\n",
      "test::: (loss) =  1.638090968132019 \t (accuracy)= 64.23220973782772 %\n",
      "Train::: epoch= 34 \t lr= 0.030052592036063103 \t (loss)= 0.013414425510027724 \t (accuracy)= 99.39633638634471 % \t time= 4949.169469833374\n",
      "test::: (loss) =  1.4223592281341553 \t (accuracy)= 67.60299625468164 %\n",
      "Train::: epoch= 35 \t lr= 0.030052592036063103 \t (loss)= 0.012010888381630568 \t (accuracy)= 99.43796835970025 % \t time= 5094.906461238861\n",
      "test::: (loss) =  1.3138216733932495 \t (accuracy)= 70.59925093632958 %\n",
      "Train::: epoch= 36 \t lr= 0.030052592036063103 \t (loss)= 0.013285262454882537 \t (accuracy)= 99.36511240632807 % \t time= 5240.642722129822\n",
      "test::: (loss) =  1.2846816778182983 \t (accuracy)= 70.78651685393258 %\n",
      "Train::: epoch= 37 \t lr= 0.030052592036063103 \t (loss)= 0.01499682578981361 \t (accuracy)= 99.30266444629476 % \t time= 5386.89417052269\n",
      "test::: (loss) =  0.8964288234710693 \t (accuracy)= 77.24719101123596 %\n",
      "Train::: epoch= 38 \t lr= 0.030052592036063103 \t (loss)= 0.010299176945658668 \t (accuracy)= 99.41715237302247 % \t time= 5532.792919158936\n",
      "test::: (loss) =  0.9434647560119629 \t (accuracy)= 78.83895131086143 %\n",
      "Train::: epoch= 39 \t lr= 0.030052592036063103 \t (loss)= 0.010027622532871433 \t (accuracy)= 99.51082431307245 % \t time= 5679.704772233963\n",
      "test::: (loss) =  0.8194223046302795 \t (accuracy)= 79.7752808988764 %\n",
      "Train::: epoch= 40 \t lr= 0.02732053821460282 \t (loss)= 0.01107541669705985 \t (accuracy)= 99.52123230641132 % \t time= 5825.449059486389\n",
      "test::: (loss) =  0.7280501127243042 \t (accuracy)= 82.30337078651685 %\n",
      "Train::: epoch= 41 \t lr= 0.02732053821460282 \t (loss)= 0.006907793436173092 \t (accuracy)= 99.5940882597835 % \t time= 5971.948865175247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test::: (loss) =  0.6103612780570984 \t (accuracy)= 84.8314606741573 %\n",
      "Train::: epoch= 42 \t lr= 0.02732053821460282 \t (loss)= 0.007147632696620369 \t (accuracy)= 99.56286427976686 % \t time= 6117.595319986343\n",
      "test::: (loss) =  0.5800907611846924 \t (accuracy)= 85.39325842696628 %\n",
      "Train::: epoch= 43 \t lr= 0.02732053821460282 \t (loss)= 0.006720383517155499 \t (accuracy)= 99.68776019983348 % \t time= 6264.2494394779205\n",
      "test::: (loss) =  0.5573002696037292 \t (accuracy)= 87.17228464419475 %\n",
      "Train::: epoch= 44 \t lr= 0.02732053821460282 \t (loss)= 0.00475455230929005 \t (accuracy)= 99.67735220649459 % \t time= 6411.304867506027\n",
      "test::: (loss) =  0.5012567639350891 \t (accuracy)= 88.38951310861424 %\n",
      "Train::: epoch= 45 \t lr= 0.02732053821460282 \t (loss)= 0.005969518075878132 \t (accuracy)= 99.65653621981681 % \t time= 6558.417688846588\n",
      "test::: (loss) =  0.5042306184768677 \t (accuracy)= 88.38951310861424 %\n",
      "Train::: epoch= 46 \t lr= 0.02732053821460282 \t (loss)= 0.00508055113972569 \t (accuracy)= 99.6669442131557 % \t time= 6706.043249130249\n",
      "test::: (loss) =  0.501751184463501 \t (accuracy)= 88.29588014981273 %\n",
      "Train::: epoch= 47 \t lr= 0.02732053821460282 \t (loss)= 0.004881950652616475 \t (accuracy)= 99.67735220649459 % \t time= 6854.699917793274\n",
      "test::: (loss) =  0.46475687623023987 \t (accuracy)= 89.32584269662921 %\n",
      "Train::: epoch= 48 \t lr= 0.02732053821460282 \t (loss)= 0.00569760273973777 \t (accuracy)= 99.6669442131557 % \t time= 7002.079962730408\n",
      "test::: (loss) =  0.43198105692863464 \t (accuracy)= 89.9812734082397 %\n",
      "Train::: epoch= 49 \t lr= 0.02732053821460282 \t (loss)= 0.0034649448129625343 \t (accuracy)= 99.73980016652789 % \t time= 7149.871058940887\n",
      "test::: (loss) =  0.4240831136703491 \t (accuracy)= 89.9812734082397 %\n",
      "Train::: epoch= 50 \t lr= 0.024836852922366197 \t (loss)= 0.003201737750175754 \t (accuracy)= 99.69816819317235 % \t time= 7297.066230773926\n",
      "test::: (loss) =  0.38802093267440796 \t (accuracy)= 91.10486891385767 %\n",
      "Train::: epoch= 51 \t lr= 0.024836852922366197 \t (loss)= 0.0025015577598270933 \t (accuracy)= 99.72939217318901 % \t time= 7444.410717487335\n",
      "test::: (loss) =  0.3622107207775116 \t (accuracy)= 91.94756554307116 %\n",
      "Train::: epoch= 52 \t lr= 0.024836852922366197 \t (loss)= 0.0025610195904604434 \t (accuracy)= 99.71898417985012 % \t time= 7590.343882083893\n",
      "test::: (loss) =  0.36721840500831604 \t (accuracy)= 91.57303370786516 %\n",
      "Train::: epoch= 53 \t lr= 0.024836852922366197 \t (loss)= 0.0030915644796958813 \t (accuracy)= 99.70857618651124 % \t time= 7736.995899438858\n",
      "test::: (loss) =  0.36773860454559326 \t (accuracy)= 91.47940074906367 %\n",
      "Train::: epoch= 54 \t lr= 0.024836852922366197 \t (loss)= 0.003795542009690288 \t (accuracy)= 99.71898417985012 % \t time= 7883.4398448467255\n",
      "test::: (loss) =  0.3369814455509186 \t (accuracy)= 92.32209737827716 %\n",
      "Train::: epoch= 55 \t lr= 0.024836852922366197 \t (loss)= 0.002059952161961356 \t (accuracy)= 99.75020815986679 % \t time= 8030.032315731049\n",
      "test::: (loss) =  0.29844930768013 \t (accuracy)= 93.63295880149812 %\n",
      "Train::: epoch= 56 \t lr= 0.024836852922366197 \t (loss)= 0.0018100337938384968 \t (accuracy)= 99.75020815986679 % \t time= 8176.688168764114\n",
      "test::: (loss) =  0.29219743609428406 \t (accuracy)= 93.53932584269663 %\n",
      "Train::: epoch= 57 \t lr= 0.024836852922366197 \t (loss)= 0.0016501661178803153 \t (accuracy)= 99.77102414654455 % \t time= 8322.79569888115\n",
      "test::: (loss) =  0.24946849048137665 \t (accuracy)= 94.47565543071161 %\n",
      "Train::: epoch= 58 \t lr= 0.024836852922366197 \t (loss)= 0.0014307382205009465 \t (accuracy)= 99.77102414654455 % \t time= 8469.86587715149\n",
      "test::: (loss) =  0.21927164494991302 \t (accuracy)= 94.85018726591761 %\n",
      "Train::: epoch= 59 \t lr= 0.024836852922366197 \t (loss)= 0.0014520432305398566 \t (accuracy)= 99.76061615320566 % \t time= 8615.943037986755\n",
      "test::: (loss) =  0.24055835604667664 \t (accuracy)= 94.28838951310861 %\n",
      "Train::: epoch= 60 \t lr= 0.022578957202151088 \t (loss)= 0.0013283410547158907 \t (accuracy)= 99.77102414654455 % \t time= 8762.707645893097\n",
      "test::: (loss) =  0.21679739654064178 \t (accuracy)= 95.0374531835206 %\n",
      "Train::: epoch= 61 \t lr= 0.022578957202151088 \t (loss)= 0.001218679110749271 \t (accuracy)= 99.77102414654455 % \t time= 8908.54148364067\n",
      "test::: (loss) =  0.1969527006149292 \t (accuracy)= 95.2247191011236 %\n",
      "Train::: epoch= 62 \t lr= 0.022578957202151088 \t (loss)= 0.001323690077767627 \t (accuracy)= 99.75020815986679 % \t time= 9055.88701224327\n",
      "test::: (loss) =  0.18446068465709686 \t (accuracy)= 95.59925093632958 %\n",
      "Train::: epoch= 63 \t lr= 0.022578957202151088 \t (loss)= 0.001282320717695275 \t (accuracy)= 99.76061615320566 % \t time= 9200.694716453552\n",
      "test::: (loss) =  0.1831292361021042 \t (accuracy)= 95.50561797752809 %\n",
      "Train::: epoch= 64 \t lr= 0.022578957202151088 \t (loss)= 0.0012288497828457365 \t (accuracy)= 99.76061615320566 % \t time= 9347.210290431976\n",
      "test::: (loss) =  0.17177848517894745 \t (accuracy)= 95.78651685393258 %\n",
      "Train::: epoch= 65 \t lr= 0.022578957202151088 \t (loss)= 0.001184162567866065 \t (accuracy)= 99.75020815986679 % \t time= 9492.556531190872\n",
      "test::: (loss) =  0.16284415125846863 \t (accuracy)= 95.78651685393258 %\n",
      "Train::: epoch= 66 \t lr= 0.022578957202151088 \t (loss)= 0.0012266818553014244 \t (accuracy)= 99.76061615320566 % \t time= 9639.02154803276\n",
      "test::: (loss) =  0.15345320105552673 \t (accuracy)= 96.16104868913857 %\n",
      "Train::: epoch= 67 \t lr= 0.022578957202151088 \t (loss)= 0.0012416204967466236 \t (accuracy)= 99.76061615320566 % \t time= 9785.862156152725\n",
      "test::: (loss) =  0.1551683098077774 \t (accuracy)= 95.97378277153558 %\n",
      "Train::: epoch= 68 \t lr= 0.022578957202151088 \t (loss)= 0.0012891112017078352 \t (accuracy)= 99.76061615320566 % \t time= 9932.829936265945\n",
      "test::: (loss) =  0.15390418469905853 \t (accuracy)= 96.16104868913857 %\n",
      "Train::: epoch= 69 \t lr= 0.022578957202151088 \t (loss)= 0.0011961719793681645 \t (accuracy)= 99.76061615320566 % \t time= 10079.593134880066\n",
      "test::: (loss) =  0.1354612410068512 \t (accuracy)= 96.44194756554307 %\n",
      "Train::: epoch= 70 \t lr= 0.02052632472922826 \t (loss)= 0.0011755981085098835 \t (accuracy)= 99.75020815986679 % \t time= 10226.536480665207\n",
      "test::: (loss) =  0.13518716394901276 \t (accuracy)= 96.44194756554307 %\n",
      "Train::: epoch= 71 \t lr= 0.02052632472922826 \t (loss)= 0.001151554711647269 \t (accuracy)= 99.75020815986679 % \t time= 10373.066280841827\n",
      "test::: (loss) =  0.13268345594406128 \t (accuracy)= 96.44194756554307 %\n",
      "Train::: epoch= 72 \t lr= 0.02052632472922826 \t (loss)= 0.0010769329504358312 \t (accuracy)= 99.76061615320566 % \t time= 10519.98732471466\n",
      "test::: (loss) =  0.1273748278617859 \t (accuracy)= 96.72284644194757 %\n",
      "Train::: epoch= 73 \t lr= 0.02052632472922826 \t (loss)= 0.0010665971538442532 \t (accuracy)= 99.76061615320566 % \t time= 10666.51991224289\n",
      "test::: (loss) =  0.11494705826044083 \t (accuracy)= 96.81647940074907 %\n",
      "Train::: epoch= 74 \t lr= 0.02052632472922826 \t (loss)= 0.0010425748920918816 \t (accuracy)= 99.76061615320566 % \t time= 10813.316472053528\n",
      "test::: (loss) =  0.10472656786441803 \t (accuracy)= 97.00374531835206 %\n",
      "Train::: epoch= 75 \t lr= 0.02052632472922826 \t (loss)= 0.0010312596958737456 \t (accuracy)= 99.76061615320566 % \t time= 10961.006803750992\n",
      "test::: (loss) =  0.10427671670913696 \t (accuracy)= 96.91011235955057 %\n",
      "Train::: epoch= 76 \t lr= 0.02052632472922826 \t (loss)= 0.0010444454786463894 \t (accuracy)= 99.76061615320566 % \t time= 11108.555794000626\n",
      "test::: (loss) =  0.10451370477676392 \t (accuracy)= 96.91011235955057 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-27c5cb3ba55e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mrun_error\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mget_error\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# backpropagate the average loss of one word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# update the weights every word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "teacher_forcing_ratio = 0.4\n",
    "# inilize variables for saving losses and accuracies \n",
    "\n",
    "accu_plt=[]\n",
    "loss_plt=[]\n",
    "test_loss_plt=[]\n",
    "accuracy_test_plt=[]\n",
    "cnt_list=[]\n",
    "for epoch in range(1,1000):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    # divide the learning rate by 1.1 each 10 epoch\n",
    "    if epoch %10==0:\n",
    "        my_lr = my_lr / 1.1\n",
    "#         torch.save(encoder, 'encoder_v2.pth') \n",
    "#         torch.save(decoder, 'decoder_v2.pth') \n",
    "    # optimizer\n",
    "    encoder_optimizer =torch.optim.SGD(encoder.parameters(), lr=my_lr)\n",
    "    decoder_optimizer =torch.optim.SGD(decoder.parameters(), lr=my_lr)\n",
    "    \n",
    "    #clear the loss for every epoch \n",
    "    total_loss=0\n",
    "    loss=0\n",
    "    loss_word=0\n",
    "    loss_char=0\n",
    "    run_error=0.0\n",
    "    \n",
    "    # chooosing diffrent samples of data every epoch\n",
    "    \n",
    "    for count in range(training_words):\n",
    "        input_tensor = trainData[count].cuda()\n",
    "        target_tensor= trainLabel[count].cuda()\n",
    "       \n",
    "\n",
    "        # input and target words length\n",
    "        input_length  = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "        \n",
    "        #initial hidden states\n",
    "        encoder_h = Variable(torch.zeros(1, 1,hidden_size))\n",
    "        encoder_c = Variable(torch.zeros(1, 1,hidden_size))\n",
    "\n",
    "        #send to GPU\n",
    "        encoder_h=encoder_h.to(device)\n",
    "        encoder_c=encoder_c.to(device)\n",
    "\n",
    "        # set the gradient values to zero \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        # Applying encoder on every character of the word  \n",
    "        # for ec in range(input_length):\n",
    "\n",
    "        encoder_out,encoder_h,encoder_c = encoder(input_tensor.view(input_length,1), encoder_h, encoder_c)\n",
    "\n",
    "\n",
    "        # applying output hidden feature of encoder as input hidden feature to decoder\n",
    "        decoder_h = encoder_h \n",
    "        decoder_c = encoder_c\n",
    "        \n",
    "        # Delivering output feature vector of the encoder into the decoder\n",
    "        decoder_input=torch.LongTensor([SOW_token]).cuda()\n",
    "        outputs = torch.zeros(input_length, vocab_size).to(device)\n",
    "        # Decoding Part\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        outputs = torch.zeros(input_length, vocab_size).to(device)\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "        # Applying Teacher forcing for the by feeding the target ouput as the next input\n",
    "        # for dc in range(target_length):\n",
    "            for dc in range(input_length):\n",
    "                decoder_out, deocder_h, decoder_c= decoder(decoder_input.view(1,1),decoder_h,decoder_c)\n",
    "\n",
    "                outputs[dc]=decoder_out\n",
    "                top1 = decoder_out[0][0].argmax()\n",
    "                decoder_input=target_tensor[dc]\n",
    "                loss+= criterion(decoder_out.view(1,vocab_size),target_tensor[dc].unsqueeze(0))   # applying teacher forcing\n",
    "            run_error+=get_error( outputs , target_tensor)\n",
    "        else: \n",
    "            for dc in range(input_length):\n",
    "                decoder_out, deocder_h, decoder_c= decoder(decoder_input.view(1,1),decoder_h,decoder_c)\n",
    "\n",
    "                outputs[dc]=decoder_out\n",
    "                top1 = decoder_out[0][0].argmax()\n",
    "                decoder_input=top1\n",
    "                loss+= criterion(decoder_out.view(1,vocab_size),target_tensor[dc].unsqueeze(0)) \n",
    "            run_error+=get_error( outputs , target_tensor)\n",
    "        # backpropagate the average loss of one word\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the weights every word\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        loss= loss.item()/input_length\n",
    "        loss_char+= loss # returning the loss across one word\n",
    "        \n",
    "    total_loss=loss_char/training_words\n",
    "    accuracy= (run_error/training_words)*100\n",
    "    accu_plt.append(accuracy)\n",
    "    loss_plt.append(total_loss)\n",
    "    elapsed = time.time()-start\n",
    "    # print('')\n",
    "    print('Train:::', 'epoch=',epoch,'\\t lr=', my_lr, '\\t (loss)=',(total_loss),'\\t (accuracy)=' , (accuracy),'%','\\t time=', elapsed)\n",
    "    \n",
    "    # evaluate on test set to monitor the loss \n",
    "    loss_tst_plt, acc_tst_plt,confusion_param =eval_on_test_set()\n",
    "    \n",
    "    # saving test parameters  \n",
    "    test_loss_plt.append(loss_tst_plt)\n",
    "    accuracy_test_plt.append(acc_tst_plt)\n",
    "    #### Saving model parameters every epoch  \n",
    "#         # counter for computing confusion matrix\n",
    "    cnt_list.append(confusion_param)\n",
    "\n",
    "    \n",
    "    save_model_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading model \n",
    "checkpoint = torch.load('seq_seq_model_sinlgle_char_50_mutation.pt')\n",
    "encoder.load_state_dict(checkpoint['encoder_model'])\n",
    "decoder.load_state_dict(checkpoint['decoder_model'])\n",
    "encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer'])\n",
    "decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer'])\n",
    "epoch = checkpoint['epoch']\n",
    "train_loss = checkpoint['train_loss']\n",
    "test_loss = checkpoint['test_loss']\n",
    "train_acc = checkpoint['train_accuracy']\n",
    "test_acc = checkpoint['test_accuracy']\n",
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
