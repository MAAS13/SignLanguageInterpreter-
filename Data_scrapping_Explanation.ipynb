{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrapping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import string\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib3\n",
    "from urllib.request import urlopen\n",
    "from IPython.core.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining th Web Page "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we  explore the page source of we our target web page, and try to find the corresponding tag and attributes of our corresponding data. Such as the following example of what is the **tags** and **attributes** that we need to target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot of the source](Capture.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the web page in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we will start reading the web page into python by using **requests** library, here we will import the web page to the object web page  as follows:</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "web_page=requests.get('https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the code above we fetch our web page from the URL, and store the output into object **web_page** object. This object contain the html code of the webpage. In the following code we try to print the 50 characters of the crowled HTML pages and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\n"
     ]
    }
   ],
   "source": [
    "print(web_page.text[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing our HTML by using Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will read the HTML into soup object (object in the class of Beautiful Soup). By this step we are try to storing the data in a sensibble structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(web_page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "records=soup.findAll('span', attrs={'class':'plainlinks'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the whole data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we correctly pinpointed the  requierd data we can run the code across the whole html page to crowl all of  our data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop across all the records\n",
    "scrap_data=[]\n",
    "letters = list(string.ascii_uppercase)\n",
    "for i in range(0,26): \n",
    "    A_Z=letters[0:26]\n",
    "    url=\"https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/\"+A_Z[i]\n",
    "    words= requests.get(url);\n",
    "    # parsing the web page \n",
    "    soup= BeautifulSoup(words.text,'html.parser')\n",
    "    # finding the records\n",
    "    records=soup.findAll('span', attrs={'class':'plainlinks'})\n",
    "    # extracting the words \n",
    "    for record in records:\n",
    "        misspell=record.find('a').text[1:-1]\n",
    "        correct_spell=record.find('a').text\n",
    "\n",
    "        scrap_data.append((A_Z[i],misspell,correct_spell))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to tabular data and export to csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Misspelled Word</th>\n",
       "      <th>Label/Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>bberan</td>\n",
       "      <td>abberant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>bberatio</td>\n",
       "      <td>abberation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>bbrieviate</td>\n",
       "      <td>abbrieviated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>bbriviate</td>\n",
       "      <td>abbriviated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>bbriviatio</td>\n",
       "      <td>abbriviation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Character Misspelled Word Label/Correct\n",
       "0             A          bberan      abberant\n",
       "1             A        bberatio    abberation\n",
       "2             A      bbrieviate  abbrieviated\n",
       "3             A       bbriviate   abbriviated\n",
       "4             A      bbriviatio  abbriviation"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(scrap_data, columns=['    Character', 'Misspelled Word', 'Label/Correct']) \n",
    "df.to_csv('Misspelled_correct_words_data.csv', index=False, encoding='utf-8')  \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary  Code of scrapping data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
